{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries\n"
      ],
      "metadata": {
        "id": "ylIH2eu9N0We"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FS39r71QnKBF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!!git clone --recursive https://github.com/multi30k/dataset.git multi30k-dataset\n",
        "!!git clone --recursive https://github.com/hyunwoongko/transformer transformer\n",
        "!!pip install torchtext==0.6.0\n",
        "!!python -m spacy download de_core_news_sm\n",
        "!!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "YwgQLvNzIeSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "os.makedirs('data/multi30k', exist_ok=True)\n",
        "os.makedirs('saved', exist_ok=True)\n",
        "os.makedirs('result', exist_ok=True)\n",
        "\n",
        "# GPU device setting\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model parameter setting\n",
        "batch_size = 128\n",
        "max_len = 256\n",
        "d_model = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "ffn_hidden = 2048\n",
        "drop_prob = 0.1\n",
        "\n",
        "# optimizer parameter setting\n",
        "init_lr = 1e-5\n",
        "factor = 0.9\n",
        "adam_eps = 5e-9\n",
        "patience = 10\n",
        "warmup = 100\n",
        "epoch = 1000\n",
        "clip = 1.0\n",
        "weight_decay = 5e-4\n",
        "inf = float('inf')"
      ],
      "metadata": {
        "id": "AIS9TUvtxqO_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "Wf2-hGvNN7-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def bleu_stats(hypothesis, reference):\n",
        "    \"\"\"Compute statistics for BLEU.\"\"\"\n",
        "    stats = []\n",
        "    stats.append(len(hypothesis))\n",
        "    stats.append(len(reference))\n",
        "    for n in range(1, 5):\n",
        "        s_ngrams = Counter(\n",
        "            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]\n",
        "        )\n",
        "        r_ngrams = Counter(\n",
        "            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]\n",
        "        )\n",
        "\n",
        "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
        "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def bleu(stats):\n",
        "    \"\"\"Compute BLEU given n-gram statistics.\"\"\"\n",
        "    if len(list(filter(lambda x: x == 0, stats))) > 0:\n",
        "        return 0\n",
        "    (c, r) = stats[:2]\n",
        "    log_bleu_prec = sum(\n",
        "        [math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]\n",
        "    ) / 4.\n",
        "    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)\n",
        "\n",
        "\n",
        "def get_bleu(hypotheses, reference):\n",
        "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
        "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    for hyp, ref in zip(hypotheses, reference):\n",
        "        stats += np.array(bleu_stats(hyp, ref))\n",
        "    return 100 * bleu(stats)\n",
        "\n",
        "\n",
        "def idx_to_word(x, vocab):\n",
        "    words = []\n",
        "    for i in x:\n",
        "        word = vocab.itos[i]\n",
        "        if '<' not in word:\n",
        "            words.append(word)\n",
        "    words = \" \".join(words)\n",
        "    return words"
      ],
      "metadata": {
        "id": "VONTvl_N-Doi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    source: Field = None\n",
        "    target: Field = None\n",
        "\n",
        "    def __init__(self, ext, tokenize_en, tokenize_de, init_token, eos_token):\n",
        "        self.ext = ext\n",
        "        self.tokenize_en = tokenize_en\n",
        "        self.tokenize_de = tokenize_de\n",
        "        self.init_token = init_token\n",
        "        self.eos_token = eos_token\n",
        "        print('dataset initializing start')\n",
        "\n",
        "    def make_dataset(self):\n",
        "        if self.ext == ('.de', '.en'):\n",
        "            self.source = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
        "                                lower=True, batch_first=True)\n",
        "            self.target = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
        "                                lower=True, batch_first=True)\n",
        "\n",
        "        elif self.ext == ('.en', '.de'):\n",
        "            self.source = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
        "                                lower=True, batch_first=True)\n",
        "            self.target = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
        "                                lower=True, batch_first=True)\n",
        "\n",
        "        train_data, valid_data, test_data = Multi30k.splits(exts=self.ext, fields=(self.source, self.target), root='data')\n",
        "        return train_data, valid_data, test_data\n",
        "\n",
        "    def build_vocab(self, train_data, min_freq):\n",
        "        self.source.build_vocab(train_data, min_freq=min_freq)\n",
        "        self.target.build_vocab(train_data, min_freq=min_freq)\n",
        "\n",
        "    def make_iter(self, train, validate, test, batch_size, device):\n",
        "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, validate, test),\n",
        "                                                                              batch_size=batch_size,\n",
        "                                                                              device=device)\n",
        "        print('dataset initializing done')\n",
        "        return train_iterator, valid_iterator, test_iterator"
      ],
      "metadata": {
        "id": "dUXIWZuh-Gyz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "PYfb4C7c-Rzi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.spacy_de = spacy.load('de_core_news_sm')\n",
        "        self.spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def tokenize_de(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes German text from a string into a list of strings\n",
        "        \"\"\"\n",
        "        return [tok.text for tok in self.spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes English text from a string into a list of strings\n",
        "        \"\"\"\n",
        "        return [tok.text for tok in self.spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "m0p9gM7kNw6n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "6BauPpdoOA5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "SEB-rrxNH3mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto Download doesn't work. Please put provided files in the following diraction: /content/data/multi30k/"
      ],
      "metadata": {
        "id": "VK9SvqtZNEOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "loader = DataLoader(ext=('.en', '.de'),\n",
        "                    tokenize_en=tokenizer.tokenize_en,\n",
        "                    tokenize_de=tokenizer.tokenize_de,\n",
        "                    init_token='<sos>',\n",
        "                    eos_token='<eos>')\n",
        "\n",
        "train, valid, test = loader.make_dataset()\n",
        "loader.build_vocab(train_data=train, min_freq=2)\n",
        "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
        "                                                     batch_size=batch_size,\n",
        "                                                     device=device)\n",
        "\n",
        "src_pad_idx = loader.source.vocab.stoi['<pad>']\n",
        "trg_pad_idx = loader.target.vocab.stoi['<pad>']\n",
        "trg_sos_idx = loader.target.vocab.stoi['<sos>']\n",
        "\n",
        "enc_voc_size = len(loader.source.vocab)\n",
        "dec_voc_size = len(loader.target.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvn5lmjtOBNz",
        "outputId": "b554e280-e5b2-43d0-e78c-b0e5749ec6a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset initializing start\n",
            "dataset initializing done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "47Sq8s7tH8ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers"
      ],
      "metadata": {
        "id": "044CyYJuJeU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ScaleDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    compute scale dot product attention\n",
        "\n",
        "    Query : given sentence that we focused on (decoder)\n",
        "    Key : every sentence to check relationship with Qeury(encoder)\n",
        "    Value : every sentence same with Key (encoder)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ScaleDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
        "        # input is 4 dimension tensor\n",
        "        # [batch_size, head, length, d_tensor]\n",
        "        batch_size, head, length, d_tensor = k.size()\n",
        "\n",
        "        # 1. dot product Query with Key^T to compute similarity\n",
        "        k_t = k.transpose(2, 3)  # transpose\n",
        "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
        "\n",
        "        # 2. apply masking (opt)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -10000)\n",
        "\n",
        "        # 3. pass them softmax to make [0, 1] range\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        # 4. multiply with Value\n",
        "        v = score @ v\n",
        "\n",
        "        return v, score"
      ],
      "metadata": {
        "id": "PCqDp3b8JlYM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = ScaleDotProductAttention()\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_concat = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # 1. dot product with weight matrices\n",
        "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
        "\n",
        "        # 2. split tensor by number of heads\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        # 3. do scale dot product to compute similarity\n",
        "        out, attention = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # 4. concat and pass to linear layer\n",
        "        out = self.concat(out)\n",
        "        out = self.w_concat(out)\n",
        "\n",
        "        # 5. visualize attention map\n",
        "        # TODO : we should implement visualization\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, tensor):\n",
        "        \"\"\"\n",
        "        split tensor by number of head\n",
        "\n",
        "        :param tensor: [batch_size, length, d_model]\n",
        "        :return: [batch_size, head, length, d_tensor]\n",
        "        \"\"\"\n",
        "        batch_size, length, d_model = tensor.size()\n",
        "\n",
        "        d_tensor = d_model // self.n_head\n",
        "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
        "        # it is similar with group convolution (split by number of heads)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def concat(self, tensor):\n",
        "        \"\"\"\n",
        "        inverse function of self.split(tensor : torch.Tensor)\n",
        "\n",
        "        :param tensor: [batch_size, head, length, d_tensor]\n",
        "        :return: [batch_size, length, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, head, length, d_tensor = tensor.size()\n",
        "        d_model = head * d_tensor\n",
        "\n",
        "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "zycoNVk9Jyrc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        var = x.var(-1, unbiased=False, keepdim=True)\n",
        "        # '-1' means last dimension.\n",
        "\n",
        "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        out = self.gamma * out + self.beta\n",
        "        return out"
      ],
      "metadata": {
        "id": "l0ONFyPhJ7zf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9h3-N88PKBei"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "oAfeP4FqKatt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    compute sinusoid encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len, device):\n",
        "        \"\"\"\n",
        "        constructor of sinusoid encoding class\n",
        "\n",
        "        :param d_model: dimension of model\n",
        "        :param max_len: max sequence length\n",
        "        :param device: hardware device setting\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # same size with input matrix (for adding with input matrix)\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        self.encoding.requires_grad = False  # we don't need to compute gradient\n",
        "\n",
        "        pos = torch.arange(0, max_len, device=device)\n",
        "        pos = pos.float().unsqueeze(dim=1)\n",
        "        # 1D => 2D unsqueeze to represent word's position\n",
        "\n",
        "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
        "        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n",
        "        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n",
        "\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
        "        # compute positional encoding to consider positional information of words\n",
        "\n",
        "    def forward(self, x):\n",
        "        # self.encoding\n",
        "        # [max_len = 512, d_model = 512]\n",
        "\n",
        "        batch_size, seq_len = x.size()\n",
        "        # [batch_size = 128, seq_len = 30]\n",
        "\n",
        "        return self.encoding[:seq_len, :]\n",
        "        # [seq_len = 30, d_model = 512]\n",
        "        # it will add with tok_emb : [128, 30, 512]"
      ],
      "metadata": {
        "id": "_kfTKp5OKaNf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TokenEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    Token Embedding using torch.nn\n",
        "    they will dense representation of word using weighted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        \"\"\"\n",
        "        class for token embedding that included positional information\n",
        "\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param d_model: dimensions of model\n",
        "        \"\"\"\n",
        "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"
      ],
      "metadata": {
        "id": "9gscgNAuKiP9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    token embedding + positional encoding (sinusoid)\n",
        "    positional encoding can give positional information to network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
        "        \"\"\"\n",
        "        class for word embedding that included positional information\n",
        "\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param d_model: dimensions of model\n",
        "        \"\"\"\n",
        "        super(TransformerEmbedding, self).__init__()\n",
        "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n",
        "        self.drop_out = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tok_emb = self.tok_emb(x)\n",
        "        pos_emb = self.pos_emb(x)\n",
        "        return self.drop_out(tok_emb + pos_emb)"
      ],
      "metadata": {
        "id": "cZ2fXmXYLn7r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Blocks"
      ],
      "metadata": {
        "id": "rMQ2xKHRJSEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # 1. compute self attention\n",
        "        _x = x\n",
        "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
        "\n",
        "        # 2. add and norm\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        # 3. positionwise feed forward network\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        # 4. add and norm\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + _x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MSkwKAd8L0-w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = LayerNorm(d_model=d_model)\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, dec, enc, trg_mask, src_mask):\n",
        "        # 1. compute self attention\n",
        "        _x = dec\n",
        "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
        "\n",
        "        # 2. add and norm\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        if enc is not None:\n",
        "            # 3. compute encoder - decoder attention\n",
        "            _x = x\n",
        "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
        "\n",
        "            # 4. add and norm\n",
        "            x = self.dropout2(x)\n",
        "            x = self.norm2(x + _x)\n",
        "\n",
        "        # 5. positionwise feed forward network\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        # 6. add and norm\n",
        "        x = self.dropout3(x)\n",
        "        x = self.norm3(x + _x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QA1uOviWJP1Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "AsftQ4cuMHzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(d_model=d_model,\n",
        "                                        max_len=max_len,\n",
        "                                        vocab_size=enc_voc_size,\n",
        "                                        drop_prob=drop_prob,\n",
        "                                        device=device)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
        "                                                  ffn_hidden=ffn_hidden,\n",
        "                                                  n_head=n_head,\n",
        "                                                  drop_prob=drop_prob)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.emb(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "AojjXj-1MClq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(d_model=d_model,\n",
        "                                        drop_prob=drop_prob,\n",
        "                                        max_len=max_len,\n",
        "                                        vocab_size=dec_voc_size,\n",
        "                                        device=device)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
        "                                                  ffn_hidden=ffn_hidden,\n",
        "                                                  n_head=n_head,\n",
        "                                                  drop_prob=drop_prob)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.emb(trg)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # pass to LM head\n",
        "        output = self.linear(trg)\n",
        "        return output"
      ],
      "metadata": {
        "id": "HfisZ8twIBl3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
        "                 ffn_hidden, n_layers, drop_prob, device):\n",
        "        super().__init__()\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.trg_sos_idx = trg_sos_idx\n",
        "        self.device = device\n",
        "        self.encoder = Encoder(d_model=d_model,\n",
        "                               n_head=n_head,\n",
        "                               max_len=max_len,\n",
        "                               ffn_hidden=ffn_hidden,\n",
        "                               enc_voc_size=enc_voc_size,\n",
        "                               drop_prob=drop_prob,\n",
        "                               n_layers=n_layers,\n",
        "                               device=device)\n",
        "\n",
        "        self.decoder = Decoder(d_model=d_model,\n",
        "                               n_head=n_head,\n",
        "                               max_len=max_len,\n",
        "                               ffn_hidden=ffn_hidden,\n",
        "                               dec_voc_size=dec_voc_size,\n",
        "                               drop_prob=drop_prob,\n",
        "                               n_layers=n_layers,\n",
        "                               device=device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask"
      ],
      "metadata": {
        "id": "QwiN7-QrMNod"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "EiEZU0hBIEJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.kaiming_uniform(m.weight.data)\n",
        "\n",
        "\n",
        "model = Transformer(src_pad_idx=src_pad_idx,\n",
        "                    trg_pad_idx=trg_pad_idx,\n",
        "                    trg_sos_idx=trg_sos_idx,\n",
        "                    d_model=d_model,\n",
        "                    enc_voc_size=enc_voc_size,\n",
        "                    dec_voc_size=dec_voc_size,\n",
        "                    max_len=max_len,\n",
        "                    ffn_hidden=ffn_hidden,\n",
        "                    n_head=n_heads,\n",
        "                    n_layers=n_layers,\n",
        "                    drop_prob=drop_prob,\n",
        "                    device=device).to(device)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.apply(initialize_weights)\n",
        "optimizer = Adam(params=model.parameters(),\n",
        "                 lr=init_lr,\n",
        "                 weight_decay=weight_decay,\n",
        "                 eps=adam_eps)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                 verbose=True,\n",
        "                                                 factor=factor,\n",
        "                                                 patience=patience)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg[:, :-1])\n",
        "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output_reshape, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    batch_bleu = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg[:, :-1])\n",
        "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output_reshape, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            total_bleu = []\n",
        "            for j in range(batch_size):\n",
        "                try:\n",
        "                    trg_words = idx_to_word(batch.trg[j], loader.target.vocab)\n",
        "                    output_words = output[j].max(dim=1)[1]\n",
        "                    output_words = idx_to_word(output_words, loader.target.vocab)\n",
        "                    bleu = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n",
        "                    total_bleu.append(bleu)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            total_bleu = sum(total_bleu) / len(total_bleu)\n",
        "            batch_bleu.append(total_bleu)\n",
        "\n",
        "    batch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
        "    return epoch_loss / len(iterator), batch_bleu\n",
        "\n",
        "\n",
        "def run(total_epoch, best_loss):\n",
        "    train_losses, test_losses, bleus = [], [], []\n",
        "    for step in range(total_epoch):\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
        "        valid_loss, bleu = evaluate(model, valid_iter, criterion)\n",
        "        end_time = time.time()\n",
        "\n",
        "        if step > warmup:\n",
        "            scheduler.step(valid_loss)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(valid_loss)\n",
        "        bleus.append(bleu)\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n",
        "\n",
        "        f = open('result/train_loss.txt', 'w')\n",
        "        f.write(str(train_losses))\n",
        "        f.close()\n",
        "\n",
        "        f = open('result/bleu.txt', 'w')\n",
        "        f.write(str(bleus))\n",
        "        f.close()\n",
        "\n",
        "        f = open('result/test_loss.txt', 'w')\n",
        "        f.write(str(test_losses))\n",
        "        f.close()\n",
        "\n",
        "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
        "        print(f'\\tBLEU Score: {bleu:.3f}')\n",
        "\n",
        "\n",
        "run(total_epoch=epoch, best_loss=inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_FjyKlFIGzj",
        "outputId": "6f259e68-132a-4776-dc78-843ed4d947e9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 55,205,037 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a05b0f21935d>:14: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  nn.init.kaiming_uniform(m.weight.data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 0.0 % , loss : 9.809916496276855\n",
            "step : 0.44 % , loss : 9.645048141479492\n",
            "step : 0.88 % , loss : 9.430861473083496\n",
            "step : 1.32 % , loss : 9.265555381774902\n",
            "step : 1.76 % , loss : 9.060550689697266\n",
            "step : 2.2 % , loss : 8.91166877746582\n",
            "step : 2.64 % , loss : 8.796218872070312\n",
            "step : 3.08 % , loss : 8.648109436035156\n",
            "step : 3.52 % , loss : 8.498396873474121\n",
            "step : 3.96 % , loss : 8.449249267578125\n",
            "step : 4.41 % , loss : 8.284452438354492\n",
            "step : 4.85 % , loss : 8.230571746826172\n",
            "step : 5.29 % , loss : 8.184290885925293\n",
            "step : 5.73 % , loss : 8.034406661987305\n",
            "step : 6.17 % , loss : 7.975549697875977\n",
            "step : 6.61 % , loss : 7.955327987670898\n",
            "step : 7.05 % , loss : 7.804980278015137\n",
            "step : 7.49 % , loss : 7.805963516235352\n",
            "step : 7.93 % , loss : 7.669694423675537\n",
            "step : 8.37 % , loss : 7.673296928405762\n",
            "step : 8.81 % , loss : 7.598231792449951\n",
            "step : 9.25 % , loss : 7.469064235687256\n",
            "step : 9.69 % , loss : 7.483203887939453\n",
            "step : 10.13 % , loss : 7.458212375640869\n",
            "step : 10.57 % , loss : 7.424996376037598\n",
            "step : 11.01 % , loss : 7.2642388343811035\n",
            "step : 11.45 % , loss : 7.356640815734863\n",
            "step : 11.89 % , loss : 7.383340835571289\n",
            "step : 12.33 % , loss : 7.237549781799316\n",
            "step : 12.78 % , loss : 7.2776994705200195\n",
            "step : 13.22 % , loss : 7.222394943237305\n",
            "step : 13.66 % , loss : 7.1505327224731445\n",
            "step : 14.1 % , loss : 7.186214447021484\n",
            "step : 14.54 % , loss : 7.186207294464111\n",
            "step : 14.98 % , loss : 7.1643500328063965\n",
            "step : 15.42 % , loss : 7.158957481384277\n",
            "step : 15.86 % , loss : 7.050866603851318\n",
            "step : 16.3 % , loss : 7.001244068145752\n",
            "step : 16.74 % , loss : 7.079044342041016\n",
            "step : 17.18 % , loss : 6.969396591186523\n",
            "step : 17.62 % , loss : 7.033815860748291\n",
            "step : 18.06 % , loss : 6.928949356079102\n",
            "step : 18.5 % , loss : 7.007957458496094\n",
            "step : 18.94 % , loss : 6.97715425491333\n",
            "step : 19.38 % , loss : 6.9043779373168945\n",
            "step : 19.82 % , loss : 6.968364715576172\n",
            "step : 20.26 % , loss : 7.070958614349365\n",
            "step : 20.7 % , loss : 6.987948417663574\n",
            "step : 21.15 % , loss : 6.878427505493164\n",
            "step : 21.59 % , loss : 6.98088264465332\n",
            "step : 22.03 % , loss : 6.86322546005249\n",
            "step : 22.47 % , loss : 6.779383182525635\n",
            "step : 22.91 % , loss : 6.901546001434326\n",
            "step : 23.35 % , loss : 6.971449851989746\n",
            "step : 23.79 % , loss : 6.74446964263916\n",
            "step : 24.23 % , loss : 6.814411163330078\n",
            "step : 24.67 % , loss : 6.7686991691589355\n",
            "step : 25.11 % , loss : 6.787715911865234\n",
            "step : 25.55 % , loss : 6.803613185882568\n",
            "step : 25.99 % , loss : 6.746695518493652\n",
            "step : 26.43 % , loss : 6.671994209289551\n",
            "step : 26.87 % , loss : 6.7988667488098145\n",
            "step : 27.31 % , loss : 6.701611518859863\n",
            "step : 27.75 % , loss : 6.732757568359375\n",
            "step : 28.19 % , loss : 6.730189800262451\n",
            "step : 28.63 % , loss : 6.820123195648193\n",
            "step : 29.07 % , loss : 6.6902337074279785\n",
            "step : 29.52 % , loss : 6.702970027923584\n",
            "step : 29.96 % , loss : 6.752465724945068\n",
            "step : 30.4 % , loss : 6.782663822174072\n",
            "step : 30.84 % , loss : 6.696397304534912\n",
            "step : 31.28 % , loss : 6.667212009429932\n",
            "step : 31.72 % , loss : 6.7175703048706055\n",
            "step : 32.16 % , loss : 6.529288291931152\n",
            "step : 32.6 % , loss : 6.699360370635986\n",
            "step : 33.04 % , loss : 6.748818874359131\n",
            "step : 33.48 % , loss : 6.750874042510986\n",
            "step : 33.92 % , loss : 6.631894111633301\n",
            "step : 34.36 % , loss : 6.669992923736572\n",
            "step : 34.8 % , loss : 6.610439777374268\n",
            "step : 35.24 % , loss : 6.596646785736084\n",
            "step : 35.68 % , loss : 6.6827592849731445\n",
            "step : 36.12 % , loss : 6.559820652008057\n",
            "step : 36.56 % , loss : 6.659882545471191\n",
            "step : 37.0 % , loss : 6.609509468078613\n",
            "step : 37.44 % , loss : 6.636967658996582\n",
            "step : 37.89 % , loss : 6.631173133850098\n",
            "step : 38.33 % , loss : 6.636831283569336\n",
            "step : 38.77 % , loss : 6.635753631591797\n",
            "step : 39.21 % , loss : 6.56269645690918\n",
            "step : 39.65 % , loss : 6.518911838531494\n",
            "step : 40.09 % , loss : 6.583399772644043\n",
            "step : 40.53 % , loss : 6.5797648429870605\n",
            "step : 40.97 % , loss : 6.644082546234131\n",
            "step : 41.41 % , loss : 6.566444396972656\n",
            "step : 41.85 % , loss : 6.503513813018799\n",
            "step : 42.29 % , loss : 6.545851707458496\n",
            "step : 42.73 % , loss : 6.55437707901001\n",
            "step : 43.17 % , loss : 6.388149738311768\n",
            "step : 43.61 % , loss : 6.508143424987793\n",
            "step : 44.05 % , loss : 6.4080491065979\n",
            "step : 44.49 % , loss : 6.444560527801514\n",
            "step : 44.93 % , loss : 6.512261867523193\n",
            "step : 45.37 % , loss : 6.494811534881592\n",
            "step : 45.81 % , loss : 6.596315860748291\n",
            "step : 46.26 % , loss : 6.4610209465026855\n",
            "step : 46.7 % , loss : 6.477968215942383\n",
            "step : 47.14 % , loss : 6.57403564453125\n",
            "step : 47.58 % , loss : 6.5463948249816895\n",
            "step : 48.02 % , loss : 6.553639888763428\n",
            "step : 48.46 % , loss : 6.555503845214844\n",
            "step : 48.9 % , loss : 6.359292030334473\n",
            "step : 49.34 % , loss : 6.515536308288574\n",
            "step : 49.78 % , loss : 6.419424533843994\n",
            "step : 50.22 % , loss : 6.551080703735352\n",
            "step : 50.66 % , loss : 6.567641735076904\n",
            "step : 51.1 % , loss : 6.445074558258057\n",
            "step : 51.54 % , loss : 6.511214733123779\n",
            "step : 51.98 % , loss : 6.534960746765137\n",
            "step : 52.42 % , loss : 6.356953144073486\n",
            "step : 52.86 % , loss : 6.487378120422363\n",
            "step : 53.3 % , loss : 6.370670318603516\n",
            "step : 53.74 % , loss : 6.419546604156494\n",
            "step : 54.19 % , loss : 6.51068639755249\n",
            "step : 54.63 % , loss : 6.461518287658691\n",
            "step : 55.07 % , loss : 6.392613887786865\n",
            "step : 55.51 % , loss : 6.432436466217041\n",
            "step : 55.95 % , loss : 6.418354034423828\n",
            "step : 56.39 % , loss : 6.500576496124268\n",
            "step : 56.83 % , loss : 6.401247024536133\n",
            "step : 57.27 % , loss : 6.394391059875488\n",
            "step : 57.71 % , loss : 6.456428050994873\n",
            "step : 58.15 % , loss : 6.440659046173096\n",
            "step : 58.59 % , loss : 6.458859920501709\n",
            "step : 59.03 % , loss : 6.415150165557861\n",
            "step : 59.47 % , loss : 6.409547805786133\n",
            "step : 59.91 % , loss : 6.4309821128845215\n",
            "step : 60.35 % , loss : 6.4348649978637695\n",
            "step : 60.79 % , loss : 6.400896072387695\n",
            "step : 61.23 % , loss : 6.452825546264648\n",
            "step : 61.67 % , loss : 6.3129563331604\n",
            "step : 62.11 % , loss : 6.442474842071533\n",
            "step : 62.56 % , loss : 6.379637718200684\n",
            "step : 63.0 % , loss : 6.323599815368652\n",
            "step : 63.44 % , loss : 6.440761089324951\n",
            "step : 63.88 % , loss : 6.419838905334473\n",
            "step : 64.32 % , loss : 6.379388332366943\n",
            "step : 64.76 % , loss : 6.386961936950684\n",
            "step : 65.2 % , loss : 6.43468713760376\n",
            "step : 65.64 % , loss : 6.307013511657715\n",
            "step : 66.08 % , loss : 6.288389205932617\n",
            "step : 66.52 % , loss : 6.3846940994262695\n",
            "step : 66.96 % , loss : 6.397449016571045\n",
            "step : 67.4 % , loss : 6.283882141113281\n",
            "step : 67.84 % , loss : 6.326401233673096\n",
            "step : 68.28 % , loss : 6.290188312530518\n",
            "step : 68.72 % , loss : 6.262833118438721\n",
            "step : 69.16 % , loss : 6.301846504211426\n",
            "step : 69.6 % , loss : 6.315653324127197\n",
            "step : 70.04 % , loss : 6.42631721496582\n",
            "step : 70.48 % , loss : 6.374390602111816\n",
            "step : 70.93 % , loss : 6.42110013961792\n",
            "step : 71.37 % , loss : 6.292951583862305\n",
            "step : 71.81 % , loss : 6.338890075683594\n",
            "step : 72.25 % , loss : 6.302088260650635\n",
            "step : 72.69 % , loss : 6.339739799499512\n",
            "step : 73.13 % , loss : 6.300364017486572\n",
            "step : 73.57 % , loss : 6.345627307891846\n",
            "step : 74.01 % , loss : 6.227703094482422\n",
            "step : 74.45 % , loss : 6.310391426086426\n",
            "step : 74.89 % , loss : 6.2437663078308105\n",
            "step : 75.33 % , loss : 6.225620746612549\n",
            "step : 75.77 % , loss : 6.239803314208984\n",
            "step : 76.21 % , loss : 6.2840423583984375\n",
            "step : 76.65 % , loss : 6.211333751678467\n",
            "step : 77.09 % , loss : 6.2251362800598145\n",
            "step : 77.53 % , loss : 6.2665019035339355\n",
            "step : 77.97 % , loss : 6.270283222198486\n",
            "step : 78.41 % , loss : 6.310648441314697\n",
            "step : 78.85 % , loss : 6.20535135269165\n",
            "step : 79.3 % , loss : 6.234404563903809\n",
            "step : 79.74 % , loss : 6.241866588592529\n",
            "step : 80.18 % , loss : 6.373547554016113\n",
            "step : 80.62 % , loss : 6.338623046875\n",
            "step : 81.06 % , loss : 6.265655517578125\n",
            "step : 81.5 % , loss : 6.265104293823242\n",
            "step : 81.94 % , loss : 6.302021026611328\n",
            "step : 82.38 % , loss : 6.185670375823975\n",
            "step : 82.82 % , loss : 6.166407585144043\n",
            "step : 83.26 % , loss : 6.200024127960205\n",
            "step : 83.7 % , loss : 6.23733377456665\n",
            "step : 84.14 % , loss : 6.209871292114258\n",
            "step : 84.58 % , loss : 6.186057090759277\n",
            "step : 85.02 % , loss : 6.2816314697265625\n",
            "step : 85.46 % , loss : 6.318196773529053\n",
            "step : 85.9 % , loss : 6.10056209564209\n",
            "step : 86.34 % , loss : 6.165975570678711\n",
            "step : 86.78 % , loss : 6.201681613922119\n",
            "step : 87.22 % , loss : 6.242307662963867\n",
            "step : 87.67 % , loss : 6.198754787445068\n",
            "step : 88.11 % , loss : 6.231140613555908\n",
            "step : 88.55 % , loss : 6.208018779754639\n",
            "step : 88.99 % , loss : 6.243072509765625\n",
            "step : 89.43 % , loss : 6.2514238357543945\n",
            "step : 89.87 % , loss : 6.206036567687988\n",
            "step : 90.31 % , loss : 6.112428665161133\n",
            "step : 90.75 % , loss : 6.2287211418151855\n",
            "step : 91.19 % , loss : 6.181291103363037\n",
            "step : 91.63 % , loss : 6.215915679931641\n",
            "step : 92.07 % , loss : 6.2169084548950195\n",
            "step : 92.51 % , loss : 6.223121643066406\n",
            "step : 92.95 % , loss : 6.133508205413818\n",
            "step : 93.39 % , loss : 6.1664581298828125\n",
            "step : 93.83 % , loss : 6.096828937530518\n",
            "step : 94.27 % , loss : 6.149662971496582\n",
            "step : 94.71 % , loss : 6.077940464019775\n",
            "step : 95.15 % , loss : 6.150114059448242\n",
            "step : 95.59 % , loss : 6.244472980499268\n",
            "step : 96.04 % , loss : 6.09243631362915\n",
            "step : 96.48 % , loss : 6.126343727111816\n",
            "step : 96.92 % , loss : 6.071204662322998\n",
            "step : 97.36 % , loss : 6.147716045379639\n",
            "step : 97.8 % , loss : 6.2041015625\n",
            "step : 98.24 % , loss : 6.149400234222412\n",
            "step : 98.68 % , loss : 6.1117706298828125\n",
            "step : 99.12 % , loss : 6.175905227661133\n",
            "step : 99.56 % , loss : 6.069051742553711\n",
            "Epoch: 1 | Time: 1m 34s\n",
            "\tTrain Loss: 6.703 | Train PPL: 814.569\n",
            "\tVal Loss: 5.944 |  Val PPL: 381.322\n",
            "\tBLEU Score: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "ZylAdSzCIHG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "\n",
        "def read(name):\n",
        "    f = open(name, 'r')\n",
        "    file = f.read()\n",
        "    file = re.sub('\\\\[', '', file)\n",
        "    file = re.sub('\\\\]', '', file)\n",
        "    f.close()\n",
        "\n",
        "    return [float(i) for idx, i in enumerate(file.split(','))]\n",
        "\n",
        "\n",
        "def draw(mode):\n",
        "    if mode == 'loss':\n",
        "        train = read('./result/train_loss.txt')\n",
        "        test = read('./result/test_loss.txt')\n",
        "        plt.plot(train, 'r', label='train')\n",
        "        plt.plot(test, 'b', label='validation')\n",
        "        plt.legend(loc='lower left')\n",
        "\n",
        "\n",
        "    elif mode == 'bleu':\n",
        "        bleu = read('./result/bleu.txt')\n",
        "        plt.plot(bleu, 'b', label='bleu score')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel(mode)\n",
        "    plt.title('training result')\n",
        "    plt.grid(True, which='both', axis='both')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "draw(mode='loss')\n",
        "draw(mode='bleu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "l3X_DcpgIKED",
        "outputId": "9d097c64-57e9-4aba-f30a-280420ec506c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}