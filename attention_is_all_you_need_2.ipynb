{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries\n"
      ],
      "metadata": {
        "id": "ylIH2eu9N0We"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FS39r71QnKBF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!!git clone --recursive https://github.com/multi30k/dataset.git multi30k-dataset\n",
        "!!git clone --recursive https://github.com/hyunwoongko/transformer transformer\n",
        "!!pip install torchtext==0.6.0\n",
        "!!python -m spacy download de_core_news_sm\n",
        "!!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "YwgQLvNzIeSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "os.makedirs('data/multi30k', exist_ok=True)\n",
        "os.makedirs('saved', exist_ok=True)\n",
        "os.makedirs('result', exist_ok=True)\n",
        "\n",
        "# GPU device setting\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model parameter setting\n",
        "batch_size = 128\n",
        "max_len = 256\n",
        "d_model = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "ffn_hidden = 2048\n",
        "drop_prob = 0.1\n",
        "\n",
        "# optimizer parameter setting\n",
        "init_lr = 1e-5\n",
        "factor = 0.9\n",
        "adam_eps = 5e-9\n",
        "patience = 10\n",
        "warmup = 100\n",
        "# epoch = 1000\n",
        "epoch = 10\n",
        "clip = 1.0\n",
        "weight_decay = 5e-4\n",
        "inf = float('inf')"
      ],
      "metadata": {
        "id": "AIS9TUvtxqO_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "Wf2-hGvNN7-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def bleu_stats(hypothesis, reference):\n",
        "    \"\"\"Compute statistics for BLEU.\"\"\"\n",
        "    stats = []\n",
        "    stats.append(len(hypothesis))\n",
        "    stats.append(len(reference))\n",
        "    for n in range(1, 5):\n",
        "        s_ngrams = Counter(\n",
        "            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]\n",
        "        )\n",
        "        r_ngrams = Counter(\n",
        "            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]\n",
        "        )\n",
        "\n",
        "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
        "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def bleu(stats):\n",
        "    \"\"\"Compute BLEU given n-gram statistics.\"\"\"\n",
        "    if len(list(filter(lambda x: x == 0, stats))) > 0:\n",
        "        return 0\n",
        "    (c, r) = stats[:2]\n",
        "    log_bleu_prec = sum(\n",
        "        [math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]\n",
        "    ) / 4.\n",
        "    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)\n",
        "\n",
        "\n",
        "def get_bleu(hypotheses, reference):\n",
        "    \"\"\"Get validation BLEU score for dev set.\"\"\"\n",
        "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    for hyp, ref in zip(hypotheses, reference):\n",
        "        stats += np.array(bleu_stats(hyp, ref))\n",
        "    return 100 * bleu(stats)\n",
        "\n",
        "\n",
        "def idx_to_word(x, vocab):\n",
        "    words = []\n",
        "    for i in x:\n",
        "        word = vocab.itos[i]\n",
        "        if '<' not in word:\n",
        "            words.append(word)\n",
        "    words = \" \".join(words)\n",
        "    return words"
      ],
      "metadata": {
        "id": "VONTvl_N-Doi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    source: Field = None\n",
        "    target: Field = None\n",
        "\n",
        "    def __init__(self, ext, tokenize_en, tokenize_de, init_token, eos_token):\n",
        "        self.ext = ext\n",
        "        self.tokenize_en = tokenize_en\n",
        "        self.tokenize_de = tokenize_de\n",
        "        self.init_token = init_token\n",
        "        self.eos_token = eos_token\n",
        "        print('dataset initializing start')\n",
        "\n",
        "    def make_dataset(self):\n",
        "        if self.ext == ('.de', '.en'):\n",
        "            self.source = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
        "                                lower=True, batch_first=True)\n",
        "            self.target = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
        "                                lower=True, batch_first=True)\n",
        "\n",
        "        elif self.ext == ('.en', '.de'):\n",
        "            self.source = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
        "                                lower=True, batch_first=True)\n",
        "            self.target = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
        "                                lower=True, batch_first=True)\n",
        "\n",
        "        train_data, valid_data, test_data = Multi30k.splits(exts=self.ext, fields=(self.source, self.target), root='data')\n",
        "        return train_data, valid_data, test_data\n",
        "\n",
        "    def build_vocab(self, train_data, min_freq):\n",
        "        self.source.build_vocab(train_data, min_freq=min_freq)\n",
        "        self.target.build_vocab(train_data, min_freq=min_freq)\n",
        "\n",
        "    def make_iter(self, train, validate, test, batch_size, device):\n",
        "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, validate, test),\n",
        "                                                                              batch_size=batch_size,\n",
        "                                                                              device=device)\n",
        "        print('dataset initializing done')\n",
        "        return train_iterator, valid_iterator, test_iterator"
      ],
      "metadata": {
        "id": "dUXIWZuh-Gyz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "PYfb4C7c-Rzi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.spacy_de = spacy.load('de_core_news_sm')\n",
        "        self.spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def tokenize_de(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes German text from a string into a list of strings\n",
        "        \"\"\"\n",
        "        return [tok.text for tok in self.spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes English text from a string into a list of strings\n",
        "        \"\"\"\n",
        "        return [tok.text for tok in self.spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "m0p9gM7kNw6n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "6BauPpdoOA5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "SEB-rrxNH3mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto Download doesn't work. Please put provided files in the following diraction: /content/data/multi30k/"
      ],
      "metadata": {
        "id": "VK9SvqtZNEOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "loader = DataLoader(ext=('.en', '.de'),\n",
        "                    tokenize_en=tokenizer.tokenize_en,\n",
        "                    tokenize_de=tokenizer.tokenize_de,\n",
        "                    init_token='<sos>',\n",
        "                    eos_token='<eos>')\n",
        "\n",
        "train, valid, test = loader.make_dataset()\n",
        "loader.build_vocab(train_data=train, min_freq=2)\n",
        "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
        "                                                     batch_size=batch_size,\n",
        "                                                     device=device)\n",
        "\n",
        "src_pad_idx = loader.source.vocab.stoi['<pad>']\n",
        "trg_pad_idx = loader.target.vocab.stoi['<pad>']\n",
        "trg_sos_idx = loader.target.vocab.stoi['<sos>']\n",
        "\n",
        "enc_voc_size = len(loader.source.vocab)\n",
        "dec_voc_size = len(loader.target.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvn5lmjtOBNz",
        "outputId": "b554e280-e5b2-43d0-e78c-b0e5749ec6a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset initializing start\n",
            "dataset initializing done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "47Sq8s7tH8ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers"
      ],
      "metadata": {
        "id": "044CyYJuJeU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ScaleDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    compute scale dot product attention\n",
        "\n",
        "    Query : given sentence that we focused on (decoder)\n",
        "    Key : every sentence to check relationship with Qeury(encoder)\n",
        "    Value : every sentence same with Key (encoder)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ScaleDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
        "        # input is 4 dimension tensor\n",
        "        # [batch_size, head, length, d_tensor]\n",
        "        batch_size, head, length, d_tensor = k.size()\n",
        "\n",
        "        # 1. dot product Query with Key^T to compute similarity\n",
        "        k_t = k.transpose(2, 3)  # transpose\n",
        "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
        "\n",
        "        # 2. apply masking (opt)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -10000)\n",
        "\n",
        "        # 3. pass them softmax to make [0, 1] range\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        # 4. multiply with Value\n",
        "        v = score @ v\n",
        "\n",
        "        return v, score"
      ],
      "metadata": {
        "id": "PCqDp3b8JlYM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = ScaleDotProductAttention()\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_concat = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # 1. dot product with weight matrices\n",
        "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
        "\n",
        "        # 2. split tensor by number of heads\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        # 3. do scale dot product to compute similarity\n",
        "        out, attention = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # 4. concat and pass to linear layer\n",
        "        out = self.concat(out)\n",
        "        out = self.w_concat(out)\n",
        "\n",
        "        # 5. visualize attention map\n",
        "        # TODO : we should implement visualization\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, tensor):\n",
        "        \"\"\"\n",
        "        split tensor by number of head\n",
        "\n",
        "        :param tensor: [batch_size, length, d_model]\n",
        "        :return: [batch_size, head, length, d_tensor]\n",
        "        \"\"\"\n",
        "        batch_size, length, d_model = tensor.size()\n",
        "\n",
        "        d_tensor = d_model // self.n_head\n",
        "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
        "        # it is similar with group convolution (split by number of heads)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def concat(self, tensor):\n",
        "        \"\"\"\n",
        "        inverse function of self.split(tensor : torch.Tensor)\n",
        "\n",
        "        :param tensor: [batch_size, head, length, d_tensor]\n",
        "        :return: [batch_size, length, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, head, length, d_tensor = tensor.size()\n",
        "        d_model = head * d_tensor\n",
        "\n",
        "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "zycoNVk9Jyrc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        var = x.var(-1, unbiased=False, keepdim=True)\n",
        "        # '-1' means last dimension.\n",
        "\n",
        "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        out = self.gamma * out + self.beta\n",
        "        return out"
      ],
      "metadata": {
        "id": "l0ONFyPhJ7zf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9h3-N88PKBei"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "oAfeP4FqKatt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    compute sinusoid encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len, device):\n",
        "        \"\"\"\n",
        "        constructor of sinusoid encoding class\n",
        "\n",
        "        :param d_model: dimension of model\n",
        "        :param max_len: max sequence length\n",
        "        :param device: hardware device setting\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # same size with input matrix (for adding with input matrix)\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        self.encoding.requires_grad = False  # we don't need to compute gradient\n",
        "\n",
        "        pos = torch.arange(0, max_len, device=device)\n",
        "        pos = pos.float().unsqueeze(dim=1)\n",
        "        # 1D => 2D unsqueeze to represent word's position\n",
        "\n",
        "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
        "        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n",
        "        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n",
        "\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
        "        # compute positional encoding to consider positional information of words\n",
        "\n",
        "    def forward(self, x):\n",
        "        # self.encoding\n",
        "        # [max_len = 512, d_model = 512]\n",
        "\n",
        "        batch_size, seq_len = x.size()\n",
        "        # [batch_size = 128, seq_len = 30]\n",
        "\n",
        "        return self.encoding[:seq_len, :]\n",
        "        # [seq_len = 30, d_model = 512]\n",
        "        # it will add with tok_emb : [128, 30, 512]"
      ],
      "metadata": {
        "id": "_kfTKp5OKaNf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TokenEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    Token Embedding using torch.nn\n",
        "    they will dense representation of word using weighted matrix\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        \"\"\"\n",
        "        class for token embedding that included positional information\n",
        "\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param d_model: dimensions of model\n",
        "        \"\"\"\n",
        "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"
      ],
      "metadata": {
        "id": "9gscgNAuKiP9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    token embedding + positional encoding (sinusoid)\n",
        "    positional encoding can give positional information to network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
        "        \"\"\"\n",
        "        class for word embedding that included positional information\n",
        "\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param d_model: dimensions of model\n",
        "        \"\"\"\n",
        "        super(TransformerEmbedding, self).__init__()\n",
        "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n",
        "        self.drop_out = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tok_emb = self.tok_emb(x)\n",
        "        pos_emb = self.pos_emb(x)\n",
        "        return self.drop_out(tok_emb + pos_emb)"
      ],
      "metadata": {
        "id": "cZ2fXmXYLn7r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Blocks"
      ],
      "metadata": {
        "id": "rMQ2xKHRJSEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # 1. compute self attention\n",
        "        _x = x\n",
        "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
        "\n",
        "        # 2. add and norm\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        # 3. positionwise feed forward network\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        # 4. add and norm\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + _x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MSkwKAd8L0-w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = LayerNorm(d_model=d_model)\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, dec, enc, trg_mask, src_mask):\n",
        "        # 1. compute self attention\n",
        "        _x = dec\n",
        "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
        "\n",
        "        # 2. add and norm\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        if enc is not None:\n",
        "            # 3. compute encoder - decoder attention\n",
        "            _x = x\n",
        "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
        "\n",
        "            # 4. add and norm\n",
        "            x = self.dropout2(x)\n",
        "            x = self.norm2(x + _x)\n",
        "\n",
        "        # 5. positionwise feed forward network\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        # 6. add and norm\n",
        "        x = self.dropout3(x)\n",
        "        x = self.norm3(x + _x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QA1uOviWJP1Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "AsftQ4cuMHzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(d_model=d_model,\n",
        "                                        max_len=max_len,\n",
        "                                        vocab_size=enc_voc_size,\n",
        "                                        drop_prob=drop_prob,\n",
        "                                        device=device)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
        "                                                  ffn_hidden=ffn_hidden,\n",
        "                                                  n_head=n_head,\n",
        "                                                  drop_prob=drop_prob)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.emb(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "AojjXj-1MClq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(d_model=d_model,\n",
        "                                        drop_prob=drop_prob,\n",
        "                                        max_len=max_len,\n",
        "                                        vocab_size=dec_voc_size,\n",
        "                                        device=device)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
        "                                                  ffn_hidden=ffn_hidden,\n",
        "                                                  n_head=n_head,\n",
        "                                                  drop_prob=drop_prob)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.emb(trg)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # pass to LM head\n",
        "        output = self.linear(trg)\n",
        "        return output"
      ],
      "metadata": {
        "id": "HfisZ8twIBl3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
        "                 ffn_hidden, n_layers, drop_prob, device):\n",
        "        super().__init__()\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.trg_sos_idx = trg_sos_idx\n",
        "        self.device = device\n",
        "        self.encoder = Encoder(d_model=d_model,\n",
        "                               n_head=n_head,\n",
        "                               max_len=max_len,\n",
        "                               ffn_hidden=ffn_hidden,\n",
        "                               enc_voc_size=enc_voc_size,\n",
        "                               drop_prob=drop_prob,\n",
        "                               n_layers=n_layers,\n",
        "                               device=device)\n",
        "\n",
        "        self.decoder = Decoder(d_model=d_model,\n",
        "                               n_head=n_head,\n",
        "                               max_len=max_len,\n",
        "                               ffn_hidden=ffn_hidden,\n",
        "                               dec_voc_size=dec_voc_size,\n",
        "                               drop_prob=drop_prob,\n",
        "                               n_layers=n_layers,\n",
        "                               device=device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask"
      ],
      "metadata": {
        "id": "QwiN7-QrMNod"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "EiEZU0hBIEJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.kaiming_uniform(m.weight.data)\n",
        "\n",
        "\n",
        "model = Transformer(src_pad_idx=src_pad_idx,\n",
        "                    trg_pad_idx=trg_pad_idx,\n",
        "                    trg_sos_idx=trg_sos_idx,\n",
        "                    d_model=d_model,\n",
        "                    enc_voc_size=enc_voc_size,\n",
        "                    dec_voc_size=dec_voc_size,\n",
        "                    max_len=max_len,\n",
        "                    ffn_hidden=ffn_hidden,\n",
        "                    n_head=n_heads,\n",
        "                    n_layers=n_layers,\n",
        "                    drop_prob=drop_prob,\n",
        "                    device=device).to(device)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.apply(initialize_weights)\n",
        "optimizer = Adam(params=model.parameters(),\n",
        "                 lr=init_lr,\n",
        "                 weight_decay=weight_decay,\n",
        "                 eps=adam_eps)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                 verbose=True,\n",
        "                                                 factor=factor,\n",
        "                                                 patience=patience)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg[:, :-1])\n",
        "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output_reshape, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    batch_bleu = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg[:, :-1])\n",
        "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output_reshape, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            total_bleu = []\n",
        "            for j in range(batch_size):\n",
        "                try:\n",
        "                    trg_words = idx_to_word(batch.trg[j], loader.target.vocab)\n",
        "                    output_words = output[j].max(dim=1)[1]\n",
        "                    output_words = idx_to_word(output_words, loader.target.vocab)\n",
        "                    bleu = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n",
        "                    total_bleu.append(bleu)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            total_bleu = sum(total_bleu) / len(total_bleu)\n",
        "            batch_bleu.append(total_bleu)\n",
        "\n",
        "    batch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
        "    return epoch_loss / len(iterator), batch_bleu\n",
        "\n",
        "\n",
        "def run(total_epoch, best_loss):\n",
        "    train_losses, test_losses, bleus = [], [], []\n",
        "    for step in range(total_epoch):\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
        "        valid_loss, bleu = evaluate(model, valid_iter, criterion)\n",
        "        end_time = time.time()\n",
        "\n",
        "        if step > warmup:\n",
        "            scheduler.step(valid_loss)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(valid_loss)\n",
        "        bleus.append(bleu)\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n",
        "\n",
        "        f = open('result/train_loss.txt', 'w')\n",
        "        f.write(str(train_losses))\n",
        "        f.close()\n",
        "\n",
        "        f = open('result/bleu.txt', 'w')\n",
        "        f.write(str(bleus))\n",
        "        f.close()\n",
        "\n",
        "        f = open('result/test_loss.txt', 'w')\n",
        "        f.write(str(test_losses))\n",
        "        f.close()\n",
        "\n",
        "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
        "        print(f'\\tBLEU Score: {bleu:.3f}')\n",
        "\n",
        "\n",
        "run(total_epoch=epoch, best_loss=inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_FjyKlFIGzj",
        "outputId": "d94f6ed9-b33e-4fd3-8d97-77738951b0cf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 55,205,037 trainable parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-77680cf86abd>:14: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  nn.init.kaiming_uniform(m.weight.data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 0.0 % , loss : 9.94096565246582\n",
            "step : 0.44 % , loss : 9.682717323303223\n",
            "step : 0.88 % , loss : 9.412014961242676\n",
            "step : 1.32 % , loss : 9.188690185546875\n",
            "step : 1.76 % , loss : 8.93525218963623\n",
            "step : 2.2 % , loss : 8.767104148864746\n",
            "step : 2.64 % , loss : 8.656194686889648\n",
            "step : 3.08 % , loss : 8.462579727172852\n",
            "step : 3.52 % , loss : 8.38346004486084\n",
            "step : 3.96 % , loss : 8.259265899658203\n",
            "step : 4.41 % , loss : 8.186149597167969\n",
            "step : 4.85 % , loss : 8.086495399475098\n",
            "step : 5.29 % , loss : 8.027200698852539\n",
            "step : 5.73 % , loss : 7.955924034118652\n",
            "step : 6.17 % , loss : 7.8688859939575195\n",
            "step : 6.61 % , loss : 7.7660346031188965\n",
            "step : 7.05 % , loss : 7.758547782897949\n",
            "step : 7.49 % , loss : 7.6934990882873535\n",
            "step : 7.93 % , loss : 7.53532600402832\n",
            "step : 8.37 % , loss : 7.583669185638428\n",
            "step : 8.81 % , loss : 7.563180923461914\n",
            "step : 9.25 % , loss : 7.483435153961182\n",
            "step : 9.69 % , loss : 7.5288872718811035\n",
            "step : 10.13 % , loss : 7.41645622253418\n",
            "step : 10.57 % , loss : 7.34092378616333\n",
            "step : 11.01 % , loss : 7.469533443450928\n",
            "step : 11.45 % , loss : 7.343288421630859\n",
            "step : 11.89 % , loss : 7.277852535247803\n",
            "step : 12.33 % , loss : 7.378727436065674\n",
            "step : 12.78 % , loss : 7.214118957519531\n",
            "step : 13.22 % , loss : 7.2583818435668945\n",
            "step : 13.66 % , loss : 7.234292984008789\n",
            "step : 14.1 % , loss : 7.200836658477783\n",
            "step : 14.54 % , loss : 7.17296028137207\n",
            "step : 14.98 % , loss : 7.235720634460449\n",
            "step : 15.42 % , loss : 7.189810276031494\n",
            "step : 15.86 % , loss : 7.101053714752197\n",
            "step : 16.3 % , loss : 7.037447929382324\n",
            "step : 16.74 % , loss : 7.133599758148193\n",
            "step : 17.18 % , loss : 7.201480865478516\n",
            "step : 17.62 % , loss : 7.096147537231445\n",
            "step : 18.06 % , loss : 7.069889545440674\n",
            "step : 18.5 % , loss : 7.025793075561523\n",
            "step : 18.94 % , loss : 7.040482521057129\n",
            "step : 19.38 % , loss : 7.026737213134766\n",
            "step : 19.82 % , loss : 6.934751987457275\n",
            "step : 20.26 % , loss : 6.914548873901367\n",
            "step : 20.7 % , loss : 6.90688419342041\n",
            "step : 21.15 % , loss : 6.935374736785889\n",
            "step : 21.59 % , loss : 6.925388813018799\n",
            "step : 22.03 % , loss : 6.972686767578125\n",
            "step : 22.47 % , loss : 6.858968257904053\n",
            "step : 22.91 % , loss : 6.939498424530029\n",
            "step : 23.35 % , loss : 6.80876350402832\n",
            "step : 23.79 % , loss : 6.923991680145264\n",
            "step : 24.23 % , loss : 6.982571125030518\n",
            "step : 24.67 % , loss : 6.804057598114014\n",
            "step : 25.11 % , loss : 6.814886569976807\n",
            "step : 25.55 % , loss : 6.907602787017822\n",
            "step : 25.99 % , loss : 6.8916730880737305\n",
            "step : 26.43 % , loss : 6.777822971343994\n",
            "step : 26.87 % , loss : 6.850615978240967\n",
            "step : 27.31 % , loss : 6.7097978591918945\n",
            "step : 27.75 % , loss : 6.793957710266113\n",
            "step : 28.19 % , loss : 6.81201696395874\n",
            "step : 28.63 % , loss : 6.7476806640625\n",
            "step : 29.07 % , loss : 6.711912155151367\n",
            "step : 29.52 % , loss : 6.72767972946167\n",
            "step : 29.96 % , loss : 6.742809295654297\n",
            "step : 30.4 % , loss : 6.705503940582275\n",
            "step : 30.84 % , loss : 6.764665603637695\n",
            "step : 31.28 % , loss : 6.747348785400391\n",
            "step : 31.72 % , loss : 6.763868808746338\n",
            "step : 32.16 % , loss : 6.618013858795166\n",
            "step : 32.6 % , loss : 6.621272087097168\n",
            "step : 33.04 % , loss : 6.784093379974365\n",
            "step : 33.48 % , loss : 6.765783786773682\n",
            "step : 33.92 % , loss : 6.753153324127197\n",
            "step : 34.36 % , loss : 6.711329460144043\n",
            "step : 34.8 % , loss : 6.686470985412598\n",
            "step : 35.24 % , loss : 6.7234416007995605\n",
            "step : 35.68 % , loss : 6.716908931732178\n",
            "step : 36.12 % , loss : 6.657871723175049\n",
            "step : 36.56 % , loss : 6.566895008087158\n",
            "step : 37.0 % , loss : 6.707906723022461\n",
            "step : 37.44 % , loss : 6.5557861328125\n",
            "step : 37.89 % , loss : 6.716066837310791\n",
            "step : 38.33 % , loss : 6.740223407745361\n",
            "step : 38.77 % , loss : 6.56551456451416\n",
            "step : 39.21 % , loss : 6.546865463256836\n",
            "step : 39.65 % , loss : 6.596678733825684\n",
            "step : 40.09 % , loss : 6.589878082275391\n",
            "step : 40.53 % , loss : 6.755908012390137\n",
            "step : 40.97 % , loss : 6.6090521812438965\n",
            "step : 41.41 % , loss : 6.64375638961792\n",
            "step : 41.85 % , loss : 6.549057483673096\n",
            "step : 42.29 % , loss : 6.58996057510376\n",
            "step : 42.73 % , loss : 6.642744064331055\n",
            "step : 43.17 % , loss : 6.671387672424316\n",
            "step : 43.61 % , loss : 6.507261276245117\n",
            "step : 44.05 % , loss : 6.587309837341309\n",
            "step : 44.49 % , loss : 6.566369533538818\n",
            "step : 44.93 % , loss : 6.453077793121338\n",
            "step : 45.37 % , loss : 6.57220458984375\n",
            "step : 45.81 % , loss : 6.579007148742676\n",
            "step : 46.26 % , loss : 6.569245338439941\n",
            "step : 46.7 % , loss : 6.564879894256592\n",
            "step : 47.14 % , loss : 6.57082986831665\n",
            "step : 47.58 % , loss : 6.609346389770508\n",
            "step : 48.02 % , loss : 6.5724382400512695\n",
            "step : 48.46 % , loss : 6.513025760650635\n",
            "step : 48.9 % , loss : 6.519173622131348\n",
            "step : 49.34 % , loss : 6.595335483551025\n",
            "step : 49.78 % , loss : 6.500187873840332\n",
            "step : 50.22 % , loss : 6.43953800201416\n",
            "step : 50.66 % , loss : 6.602639198303223\n",
            "step : 51.1 % , loss : 6.592719078063965\n",
            "step : 51.54 % , loss : 6.531312942504883\n",
            "step : 51.98 % , loss : 6.548505783081055\n",
            "step : 52.42 % , loss : 6.495223045349121\n",
            "step : 52.86 % , loss : 6.612055778503418\n",
            "step : 53.3 % , loss : 6.423586845397949\n",
            "step : 53.74 % , loss : 6.579936504364014\n",
            "step : 54.19 % , loss : 6.577119827270508\n",
            "step : 54.63 % , loss : 6.516138076782227\n",
            "step : 55.07 % , loss : 6.444326400756836\n",
            "step : 55.51 % , loss : 6.472916126251221\n",
            "step : 55.95 % , loss : 6.401335716247559\n",
            "step : 56.39 % , loss : 6.427023887634277\n",
            "step : 56.83 % , loss : 6.463106632232666\n",
            "step : 57.27 % , loss : 6.488217830657959\n",
            "step : 57.71 % , loss : 6.515613079071045\n",
            "step : 58.15 % , loss : 6.5444231033325195\n",
            "step : 58.59 % , loss : 6.436943054199219\n",
            "step : 59.03 % , loss : 6.518421173095703\n",
            "step : 59.47 % , loss : 6.384327411651611\n",
            "step : 59.91 % , loss : 6.457610607147217\n",
            "step : 60.35 % , loss : 6.427640438079834\n",
            "step : 60.79 % , loss : 6.425251007080078\n",
            "step : 61.23 % , loss : 6.365149021148682\n",
            "step : 61.67 % , loss : 6.350329399108887\n",
            "step : 62.11 % , loss : 6.429551601409912\n",
            "step : 62.56 % , loss : 6.448826313018799\n",
            "step : 63.0 % , loss : 6.404215335845947\n",
            "step : 63.44 % , loss : 6.367964267730713\n",
            "step : 63.88 % , loss : 6.430794715881348\n",
            "step : 64.32 % , loss : 6.3910722732543945\n",
            "step : 64.76 % , loss : 6.466176509857178\n",
            "step : 65.2 % , loss : 6.428429126739502\n",
            "step : 65.64 % , loss : 6.460055351257324\n",
            "step : 66.08 % , loss : 6.473901748657227\n",
            "step : 66.52 % , loss : 6.393091201782227\n",
            "step : 66.96 % , loss : 6.260589122772217\n",
            "step : 67.4 % , loss : 6.291653156280518\n",
            "step : 67.84 % , loss : 6.409005165100098\n",
            "step : 68.28 % , loss : 6.369332790374756\n",
            "step : 68.72 % , loss : 6.343952655792236\n",
            "step : 69.16 % , loss : 6.3461432456970215\n",
            "step : 69.6 % , loss : 6.30422306060791\n",
            "step : 70.04 % , loss : 6.3548903465271\n",
            "step : 70.48 % , loss : 6.335670471191406\n",
            "step : 70.93 % , loss : 6.28809118270874\n",
            "step : 71.37 % , loss : 6.426327228546143\n",
            "step : 71.81 % , loss : 6.337854862213135\n",
            "step : 72.25 % , loss : 6.210781574249268\n",
            "step : 72.69 % , loss : 6.363467693328857\n",
            "step : 73.13 % , loss : 6.366866588592529\n",
            "step : 73.57 % , loss : 6.317876815795898\n",
            "step : 74.01 % , loss : 6.331092834472656\n",
            "step : 74.45 % , loss : 6.425989151000977\n",
            "step : 74.89 % , loss : 6.383029460906982\n",
            "step : 75.33 % , loss : 6.303599834442139\n",
            "step : 75.77 % , loss : 6.33743953704834\n",
            "step : 76.21 % , loss : 6.311026573181152\n",
            "step : 76.65 % , loss : 6.439761638641357\n",
            "step : 77.09 % , loss : 6.26383638381958\n",
            "step : 77.53 % , loss : 6.305059432983398\n",
            "step : 77.97 % , loss : 6.32050085067749\n",
            "step : 78.41 % , loss : 6.332179546356201\n",
            "step : 78.85 % , loss : 6.199514389038086\n",
            "step : 79.3 % , loss : 6.283378601074219\n",
            "step : 79.74 % , loss : 6.310232639312744\n",
            "step : 80.18 % , loss : 6.3621673583984375\n",
            "step : 80.62 % , loss : 6.229475021362305\n",
            "step : 81.06 % , loss : 6.2480149269104\n",
            "step : 81.5 % , loss : 6.231426239013672\n",
            "step : 81.94 % , loss : 6.202033996582031\n",
            "step : 82.38 % , loss : 6.241797924041748\n",
            "step : 82.82 % , loss : 6.264027118682861\n",
            "step : 83.26 % , loss : 6.327204704284668\n",
            "step : 83.7 % , loss : 6.311433792114258\n",
            "step : 84.14 % , loss : 6.2702412605285645\n",
            "step : 84.58 % , loss : 6.166434288024902\n",
            "step : 85.02 % , loss : 6.3393449783325195\n",
            "step : 85.46 % , loss : 6.202268123626709\n",
            "step : 85.9 % , loss : 6.3626532554626465\n",
            "step : 86.34 % , loss : 6.289025783538818\n",
            "step : 86.78 % , loss : 6.196127891540527\n",
            "step : 87.22 % , loss : 6.254030227661133\n",
            "step : 87.67 % , loss : 6.250925540924072\n",
            "step : 88.11 % , loss : 6.189672946929932\n",
            "step : 88.55 % , loss : 6.203130722045898\n",
            "step : 88.99 % , loss : 6.29669713973999\n",
            "step : 89.43 % , loss : 6.264288425445557\n",
            "step : 89.87 % , loss : 6.246843338012695\n",
            "step : 90.31 % , loss : 6.260936737060547\n",
            "step : 90.75 % , loss : 6.240659713745117\n",
            "step : 91.19 % , loss : 6.198435306549072\n",
            "step : 91.63 % , loss : 6.15886926651001\n",
            "step : 92.07 % , loss : 6.293410301208496\n",
            "step : 92.51 % , loss : 6.190918922424316\n",
            "step : 92.95 % , loss : 6.13921594619751\n",
            "step : 93.39 % , loss : 6.189576148986816\n",
            "step : 93.83 % , loss : 6.371540546417236\n",
            "step : 94.27 % , loss : 6.278094291687012\n",
            "step : 94.71 % , loss : 6.205587387084961\n",
            "step : 95.15 % , loss : 6.194608211517334\n",
            "step : 95.59 % , loss : 6.210619926452637\n",
            "step : 96.04 % , loss : 6.281505584716797\n",
            "step : 96.48 % , loss : 6.1941938400268555\n",
            "step : 96.92 % , loss : 6.15944766998291\n",
            "step : 97.36 % , loss : 6.118125915527344\n",
            "step : 97.8 % , loss : 6.2110276222229\n",
            "step : 98.24 % , loss : 6.133995532989502\n",
            "step : 98.68 % , loss : 6.200692176818848\n",
            "step : 99.12 % , loss : 6.233529567718506\n",
            "step : 99.56 % , loss : 6.212392807006836\n",
            "Epoch: 1 | Time: 1m 34s\n",
            "\tTrain Loss: 6.732 | Train PPL: 838.844\n",
            "\tVal Loss: 5.997 |  Val PPL: 402.245\n",
            "\tBLEU Score: 0.000\n",
            "step : 0.0 % , loss : 6.131265640258789\n",
            "step : 0.44 % , loss : 6.147274494171143\n",
            "step : 0.88 % , loss : 6.167947292327881\n",
            "step : 1.32 % , loss : 6.2510271072387695\n",
            "step : 1.76 % , loss : 6.178282737731934\n",
            "step : 2.2 % , loss : 6.119515895843506\n",
            "step : 2.64 % , loss : 6.179024696350098\n",
            "step : 3.08 % , loss : 6.167826175689697\n",
            "step : 3.52 % , loss : 6.221311092376709\n",
            "step : 3.96 % , loss : 6.177674293518066\n",
            "step : 4.41 % , loss : 6.126272201538086\n",
            "step : 4.85 % , loss : 6.160633087158203\n",
            "step : 5.29 % , loss : 6.1456427574157715\n",
            "step : 5.73 % , loss : 6.2126007080078125\n",
            "step : 6.17 % , loss : 6.134319305419922\n",
            "step : 6.61 % , loss : 6.117214679718018\n",
            "step : 7.05 % , loss : 6.152649879455566\n",
            "step : 7.49 % , loss : 6.13276481628418\n",
            "step : 7.93 % , loss : 6.151349067687988\n",
            "step : 8.37 % , loss : 6.182140827178955\n",
            "step : 8.81 % , loss : 6.152629852294922\n",
            "step : 9.25 % , loss : 6.147572040557861\n",
            "step : 9.69 % , loss : 6.0872883796691895\n",
            "step : 10.13 % , loss : 6.0800089836120605\n",
            "step : 10.57 % , loss : 6.072764873504639\n",
            "step : 11.01 % , loss : 6.187378406524658\n",
            "step : 11.45 % , loss : 5.994999408721924\n",
            "step : 11.89 % , loss : 6.07533597946167\n",
            "step : 12.33 % , loss : 6.072051048278809\n",
            "step : 12.78 % , loss : 6.033143520355225\n",
            "step : 13.22 % , loss : 6.112301826477051\n",
            "step : 13.66 % , loss : 6.092691421508789\n",
            "step : 14.1 % , loss : 5.991225242614746\n",
            "step : 14.54 % , loss : 6.250222682952881\n",
            "step : 14.98 % , loss : 6.141935348510742\n",
            "step : 15.42 % , loss : 6.094051361083984\n",
            "step : 15.86 % , loss : 6.042549133300781\n",
            "step : 16.3 % , loss : 6.12375545501709\n",
            "step : 16.74 % , loss : 6.057014465332031\n",
            "step : 17.18 % , loss : 6.149872779846191\n",
            "step : 17.62 % , loss : 6.0933685302734375\n",
            "step : 18.06 % , loss : 6.071134090423584\n",
            "step : 18.5 % , loss : 6.133300304412842\n",
            "step : 18.94 % , loss : 6.026137351989746\n",
            "step : 19.38 % , loss : 6.174439907073975\n",
            "step : 19.82 % , loss : 6.139980316162109\n",
            "step : 20.26 % , loss : 5.949157238006592\n",
            "step : 20.7 % , loss : 6.111774444580078\n",
            "step : 21.15 % , loss : 5.992752552032471\n",
            "step : 21.59 % , loss : 6.042313575744629\n",
            "step : 22.03 % , loss : 6.1169562339782715\n",
            "step : 22.47 % , loss : 6.0755438804626465\n",
            "step : 22.91 % , loss : 6.0366621017456055\n",
            "step : 23.35 % , loss : 6.1862406730651855\n",
            "step : 23.79 % , loss : 5.951683521270752\n",
            "step : 24.23 % , loss : 6.1070237159729\n",
            "step : 24.67 % , loss : 6.14168643951416\n",
            "step : 25.11 % , loss : 6.07417106628418\n",
            "step : 25.55 % , loss : 6.185366153717041\n",
            "step : 25.99 % , loss : 6.121948719024658\n",
            "step : 26.43 % , loss : 6.060084342956543\n",
            "step : 26.87 % , loss : 6.087656021118164\n",
            "step : 27.31 % , loss : 6.00057315826416\n",
            "step : 27.75 % , loss : 6.0763678550720215\n",
            "step : 28.19 % , loss : 6.094162940979004\n",
            "step : 28.63 % , loss : 6.04505729675293\n",
            "step : 29.07 % , loss : 6.097926139831543\n",
            "step : 29.52 % , loss : 6.16152811050415\n",
            "step : 29.96 % , loss : 6.013484001159668\n",
            "step : 30.4 % , loss : 5.991410732269287\n",
            "step : 30.84 % , loss : 6.018164157867432\n",
            "step : 31.28 % , loss : 6.029553413391113\n",
            "step : 31.72 % , loss : 5.918485164642334\n",
            "step : 32.16 % , loss : 5.995737075805664\n",
            "step : 32.6 % , loss : 6.066269874572754\n",
            "step : 33.04 % , loss : 6.054014205932617\n",
            "step : 33.48 % , loss : 5.97559118270874\n",
            "step : 33.92 % , loss : 6.080624103546143\n",
            "step : 34.36 % , loss : 6.008666515350342\n",
            "step : 34.8 % , loss : 5.989570617675781\n",
            "step : 35.24 % , loss : 5.927164554595947\n",
            "step : 35.68 % , loss : 6.0893778800964355\n",
            "step : 36.12 % , loss : 6.065572738647461\n",
            "step : 36.56 % , loss : 5.926982402801514\n",
            "step : 37.0 % , loss : 6.009886741638184\n",
            "step : 37.44 % , loss : 5.995270729064941\n",
            "step : 37.89 % , loss : 6.0503830909729\n",
            "step : 38.33 % , loss : 6.09049654006958\n",
            "step : 38.77 % , loss : 5.9879150390625\n",
            "step : 39.21 % , loss : 5.998096466064453\n",
            "step : 39.65 % , loss : 6.000156402587891\n",
            "step : 40.09 % , loss : 5.990636825561523\n",
            "step : 40.53 % , loss : 6.071545124053955\n",
            "step : 40.97 % , loss : 5.9983134269714355\n",
            "step : 41.41 % , loss : 5.985360622406006\n",
            "step : 41.85 % , loss : 5.968708038330078\n",
            "step : 42.29 % , loss : 5.97273063659668\n",
            "step : 42.73 % , loss : 5.9282965660095215\n",
            "step : 43.17 % , loss : 5.989444732666016\n",
            "step : 43.61 % , loss : 6.037325859069824\n",
            "step : 44.05 % , loss : 5.972620010375977\n",
            "step : 44.49 % , loss : 6.004951477050781\n",
            "step : 44.93 % , loss : 5.997943878173828\n",
            "step : 45.37 % , loss : 5.979781627655029\n",
            "step : 45.81 % , loss : 5.957950115203857\n",
            "step : 46.26 % , loss : 5.9897847175598145\n",
            "step : 46.7 % , loss : 6.0026092529296875\n",
            "step : 47.14 % , loss : 6.0071563720703125\n",
            "step : 47.58 % , loss : 5.960973739624023\n",
            "step : 48.02 % , loss : 5.91883659362793\n",
            "step : 48.46 % , loss : 5.910409450531006\n",
            "step : 48.9 % , loss : 5.971556186676025\n",
            "step : 49.34 % , loss : 5.98390007019043\n",
            "step : 49.78 % , loss : 5.92122745513916\n",
            "step : 50.22 % , loss : 5.994959831237793\n",
            "step : 50.66 % , loss : 5.992788791656494\n",
            "step : 51.1 % , loss : 5.918555736541748\n",
            "step : 51.54 % , loss : 5.97572135925293\n",
            "step : 51.98 % , loss : 5.9687066078186035\n",
            "step : 52.42 % , loss : 5.985557556152344\n",
            "step : 52.86 % , loss : 6.044974327087402\n",
            "step : 53.3 % , loss : 5.969982147216797\n",
            "step : 53.74 % , loss : 5.980485439300537\n",
            "step : 54.19 % , loss : 5.866661548614502\n",
            "step : 54.63 % , loss : 5.965509414672852\n",
            "step : 55.07 % , loss : 5.841464042663574\n",
            "step : 55.51 % , loss : 5.940500736236572\n",
            "step : 55.95 % , loss : 5.987534523010254\n",
            "step : 56.39 % , loss : 5.990172386169434\n",
            "step : 56.83 % , loss : 5.874201774597168\n",
            "step : 57.27 % , loss : 5.99694299697876\n",
            "step : 57.71 % , loss : 5.955382823944092\n",
            "step : 58.15 % , loss : 5.969637870788574\n",
            "step : 58.59 % , loss : 5.83575963973999\n",
            "step : 59.03 % , loss : 5.938642501831055\n",
            "step : 59.47 % , loss : 5.93347692489624\n",
            "step : 59.91 % , loss : 5.9007158279418945\n",
            "step : 60.35 % , loss : 5.801213264465332\n",
            "step : 60.79 % , loss : 5.887940883636475\n",
            "step : 61.23 % , loss : 6.018360137939453\n",
            "step : 61.67 % , loss : 5.960300445556641\n",
            "step : 62.11 % , loss : 5.9361701011657715\n",
            "step : 62.56 % , loss : 5.857271194458008\n",
            "step : 63.0 % , loss : 5.8905229568481445\n",
            "step : 63.44 % , loss : 5.957764625549316\n",
            "step : 63.88 % , loss : 5.884533882141113\n",
            "step : 64.32 % , loss : 5.854873180389404\n",
            "step : 64.76 % , loss : 5.95598840713501\n",
            "step : 65.2 % , loss : 5.854118347167969\n",
            "step : 65.64 % , loss : 5.958761692047119\n",
            "step : 66.08 % , loss : 5.917561054229736\n",
            "step : 66.52 % , loss : 5.961879730224609\n",
            "step : 66.96 % , loss : 5.857813835144043\n",
            "step : 67.4 % , loss : 5.912306308746338\n",
            "step : 67.84 % , loss : 5.993542194366455\n",
            "step : 68.28 % , loss : 5.876187324523926\n",
            "step : 68.72 % , loss : 5.934633731842041\n",
            "step : 69.16 % , loss : 5.931938171386719\n",
            "step : 69.6 % , loss : 5.9296064376831055\n",
            "step : 70.04 % , loss : 5.8785247802734375\n",
            "step : 70.48 % , loss : 5.940057277679443\n",
            "step : 70.93 % , loss : 5.838774681091309\n",
            "step : 71.37 % , loss : 5.819945335388184\n",
            "step : 71.81 % , loss : 5.843115329742432\n",
            "step : 72.25 % , loss : 5.817245006561279\n",
            "step : 72.69 % , loss : 5.799187183380127\n",
            "step : 73.13 % , loss : 5.926988124847412\n",
            "step : 73.57 % , loss : 5.876704692840576\n",
            "step : 74.01 % , loss : 5.931392669677734\n",
            "step : 74.45 % , loss : 5.815102577209473\n",
            "step : 74.89 % , loss : 5.959805488586426\n",
            "step : 75.33 % , loss : 5.858989238739014\n",
            "step : 75.77 % , loss : 5.83510160446167\n",
            "step : 76.21 % , loss : 5.7849836349487305\n",
            "step : 76.65 % , loss : 5.900752067565918\n",
            "step : 77.09 % , loss : 5.88635778427124\n",
            "step : 77.53 % , loss : 5.8464274406433105\n",
            "step : 77.97 % , loss : 5.84965705871582\n",
            "step : 78.41 % , loss : 5.922698020935059\n",
            "step : 78.85 % , loss : 5.865931510925293\n",
            "step : 79.3 % , loss : 5.826844692230225\n",
            "step : 79.74 % , loss : 5.828653812408447\n",
            "step : 80.18 % , loss : 5.985335350036621\n",
            "step : 80.62 % , loss : 5.863641262054443\n",
            "step : 81.06 % , loss : 5.8953070640563965\n",
            "step : 81.5 % , loss : 5.871368408203125\n",
            "step : 81.94 % , loss : 5.797418117523193\n",
            "step : 82.38 % , loss : 5.922554969787598\n",
            "step : 82.82 % , loss : 5.728164196014404\n",
            "step : 83.26 % , loss : 5.866438865661621\n",
            "step : 83.7 % , loss : 5.871119022369385\n",
            "step : 84.14 % , loss : 5.828787326812744\n",
            "step : 84.58 % , loss : 5.811450004577637\n",
            "step : 85.02 % , loss : 5.831490993499756\n",
            "step : 85.46 % , loss : 5.868322849273682\n",
            "step : 85.9 % , loss : 5.806382656097412\n",
            "step : 86.34 % , loss : 5.726864814758301\n",
            "step : 86.78 % , loss : 5.819903373718262\n",
            "step : 87.22 % , loss : 5.704569339752197\n",
            "step : 87.67 % , loss : 5.859695911407471\n",
            "step : 88.11 % , loss : 5.878495216369629\n",
            "step : 88.55 % , loss : 5.736866474151611\n",
            "step : 88.99 % , loss : 5.790060043334961\n",
            "step : 89.43 % , loss : 5.776778697967529\n",
            "step : 89.87 % , loss : 5.82785177230835\n",
            "step : 90.31 % , loss : 5.716182708740234\n",
            "step : 90.75 % , loss : 5.744730472564697\n",
            "step : 91.19 % , loss : 5.873388767242432\n",
            "step : 91.63 % , loss : 5.763411045074463\n",
            "step : 92.07 % , loss : 5.809233665466309\n",
            "step : 92.51 % , loss : 5.794602870941162\n",
            "step : 92.95 % , loss : 5.756170272827148\n",
            "step : 93.39 % , loss : 5.8528313636779785\n",
            "step : 93.83 % , loss : 5.702215671539307\n",
            "step : 94.27 % , loss : 5.778301239013672\n",
            "step : 94.71 % , loss : 5.6671905517578125\n",
            "step : 95.15 % , loss : 5.820672512054443\n",
            "step : 95.59 % , loss : 5.843624591827393\n",
            "step : 96.04 % , loss : 5.599570274353027\n",
            "step : 96.48 % , loss : 5.705793380737305\n",
            "step : 96.92 % , loss : 5.720677375793457\n",
            "step : 97.36 % , loss : 5.821533203125\n",
            "step : 97.8 % , loss : 5.71603536605835\n",
            "step : 98.24 % , loss : 5.6055216789245605\n",
            "step : 98.68 % , loss : 5.674840450286865\n",
            "step : 99.12 % , loss : 5.740097999572754\n",
            "step : 99.56 % , loss : 5.871679782867432\n",
            "Epoch: 2 | Time: 1m 34s\n",
            "\tTrain Loss: 5.964 | Train PPL: 389.180\n",
            "\tVal Loss: 6.057 |  Val PPL: 427.007\n",
            "\tBLEU Score: 0.761\n",
            "step : 0.0 % , loss : 5.822995185852051\n",
            "step : 0.44 % , loss : 5.687347412109375\n",
            "step : 0.88 % , loss : 5.7279510498046875\n",
            "step : 1.32 % , loss : 5.664968490600586\n",
            "step : 1.76 % , loss : 5.746541500091553\n",
            "step : 2.2 % , loss : 5.634488105773926\n",
            "step : 2.64 % , loss : 5.764209270477295\n",
            "step : 3.08 % , loss : 5.782278060913086\n",
            "step : 3.52 % , loss : 5.696435928344727\n",
            "step : 3.96 % , loss : 5.706397533416748\n",
            "step : 4.41 % , loss : 5.703457355499268\n",
            "step : 4.85 % , loss : 5.711892127990723\n",
            "step : 5.29 % , loss : 5.718557834625244\n",
            "step : 5.73 % , loss : 5.668118953704834\n",
            "step : 6.17 % , loss : 5.703402519226074\n",
            "step : 6.61 % , loss : 5.715829849243164\n",
            "step : 7.05 % , loss : 5.703188419342041\n",
            "step : 7.49 % , loss : 5.7206292152404785\n",
            "step : 7.93 % , loss : 5.712554931640625\n",
            "step : 8.37 % , loss : 5.692012310028076\n",
            "step : 8.81 % , loss : 5.622863292694092\n",
            "step : 9.25 % , loss : 5.688574314117432\n",
            "step : 9.69 % , loss : 5.667084217071533\n",
            "step : 10.13 % , loss : 5.786856174468994\n",
            "step : 10.57 % , loss : 5.677430629730225\n",
            "step : 11.01 % , loss : 5.744917392730713\n",
            "step : 11.45 % , loss : 5.643499374389648\n",
            "step : 11.89 % , loss : 5.628719329833984\n",
            "step : 12.33 % , loss : 5.5597968101501465\n",
            "step : 12.78 % , loss : 5.6621856689453125\n",
            "step : 13.22 % , loss : 5.6838459968566895\n",
            "step : 13.66 % , loss : 5.625552177429199\n",
            "step : 14.1 % , loss : 5.609360694885254\n",
            "step : 14.54 % , loss : 5.688479900360107\n",
            "step : 14.98 % , loss : 5.718716144561768\n",
            "step : 15.42 % , loss : 5.672296047210693\n",
            "step : 15.86 % , loss : 5.6505327224731445\n",
            "step : 16.3 % , loss : 5.552807331085205\n",
            "step : 16.74 % , loss : 5.61712646484375\n",
            "step : 17.18 % , loss : 5.692935466766357\n",
            "step : 17.62 % , loss : 5.570361614227295\n",
            "step : 18.06 % , loss : 5.754924774169922\n",
            "step : 18.5 % , loss : 5.6244025230407715\n",
            "step : 18.94 % , loss : 5.639292240142822\n",
            "step : 19.38 % , loss : 5.64764928817749\n",
            "step : 19.82 % , loss : 5.6005096435546875\n",
            "step : 20.26 % , loss : 5.784096717834473\n",
            "step : 20.7 % , loss : 5.666392803192139\n",
            "step : 21.15 % , loss : 5.6233439445495605\n",
            "step : 21.59 % , loss : 5.664031505584717\n",
            "step : 22.03 % , loss : 5.66966438293457\n",
            "step : 22.47 % , loss : 5.731786251068115\n",
            "step : 22.91 % , loss : 5.601132392883301\n",
            "step : 23.35 % , loss : 5.6215620040893555\n",
            "step : 23.79 % , loss : 5.682723045349121\n",
            "step : 24.23 % , loss : 5.663968086242676\n",
            "step : 24.67 % , loss : 5.665282726287842\n",
            "step : 25.11 % , loss : 5.7286787033081055\n",
            "step : 25.55 % , loss : 5.590201377868652\n",
            "step : 25.99 % , loss : 5.557353973388672\n",
            "step : 26.43 % , loss : 5.666326522827148\n",
            "step : 26.87 % , loss : 5.5663228034973145\n",
            "step : 27.31 % , loss : 5.580833435058594\n",
            "step : 27.75 % , loss : 5.657673358917236\n",
            "step : 28.19 % , loss : 5.572554111480713\n",
            "step : 28.63 % , loss : 5.602957725524902\n",
            "step : 29.07 % , loss : 5.696026802062988\n",
            "step : 29.52 % , loss : 5.651186943054199\n",
            "step : 29.96 % , loss : 5.603562831878662\n",
            "step : 30.4 % , loss : 5.579470157623291\n",
            "step : 30.84 % , loss : 5.616262912750244\n",
            "step : 31.28 % , loss : 5.634657859802246\n",
            "step : 31.72 % , loss : 5.555462837219238\n",
            "step : 32.16 % , loss : 5.635773658752441\n",
            "step : 32.6 % , loss : 5.596379280090332\n",
            "step : 33.04 % , loss : 5.7291998863220215\n",
            "step : 33.48 % , loss : 5.585505962371826\n",
            "step : 33.92 % , loss : 5.73820686340332\n",
            "step : 34.36 % , loss : 5.6328935623168945\n",
            "step : 34.8 % , loss : 5.761165142059326\n",
            "step : 35.24 % , loss : 5.583856105804443\n",
            "step : 35.68 % , loss : 5.649156093597412\n",
            "step : 36.12 % , loss : 5.632968425750732\n",
            "step : 36.56 % , loss : 5.579017162322998\n",
            "step : 37.0 % , loss : 5.544251918792725\n",
            "step : 37.44 % , loss : 5.585295677185059\n",
            "step : 37.89 % , loss : 5.571097373962402\n",
            "step : 38.33 % , loss : 5.577791690826416\n",
            "step : 38.77 % , loss : 5.603328704833984\n",
            "step : 39.21 % , loss : 5.573400497436523\n",
            "step : 39.65 % , loss : 5.572203159332275\n",
            "step : 40.09 % , loss : 5.600151062011719\n",
            "step : 40.53 % , loss : 5.6431684494018555\n",
            "step : 40.97 % , loss : 5.558014392852783\n",
            "step : 41.41 % , loss : 5.662071228027344\n",
            "step : 41.85 % , loss : 5.6570539474487305\n",
            "step : 42.29 % , loss : 5.640537261962891\n",
            "step : 42.73 % , loss : 5.630738735198975\n",
            "step : 43.17 % , loss : 5.652312278747559\n",
            "step : 43.61 % , loss : 5.589714527130127\n",
            "step : 44.05 % , loss : 5.525008678436279\n",
            "step : 44.49 % , loss : 5.588878631591797\n",
            "step : 44.93 % , loss : 5.584425926208496\n",
            "step : 45.37 % , loss : 5.590976715087891\n",
            "step : 45.81 % , loss : 5.588972091674805\n",
            "step : 46.26 % , loss : 5.562029838562012\n",
            "step : 46.7 % , loss : 5.611287593841553\n",
            "step : 47.14 % , loss : 5.71904993057251\n",
            "step : 47.58 % , loss : 5.564685821533203\n",
            "step : 48.02 % , loss : 5.5490922927856445\n",
            "step : 48.46 % , loss : 5.53629207611084\n",
            "step : 48.9 % , loss : 5.549853801727295\n",
            "step : 49.34 % , loss : 5.513874053955078\n",
            "step : 49.78 % , loss : 5.603671550750732\n",
            "step : 50.22 % , loss : 5.610223293304443\n",
            "step : 50.66 % , loss : 5.616977691650391\n",
            "step : 51.1 % , loss : 5.526264190673828\n",
            "step : 51.54 % , loss : 5.636165618896484\n",
            "step : 51.98 % , loss : 5.650131702423096\n",
            "step : 52.42 % , loss : 5.59750509262085\n",
            "step : 52.86 % , loss : 5.577088356018066\n",
            "step : 53.3 % , loss : 5.567146301269531\n",
            "step : 53.74 % , loss : 5.52807092666626\n",
            "step : 54.19 % , loss : 5.487259864807129\n",
            "step : 54.63 % , loss : 5.5626325607299805\n",
            "step : 55.07 % , loss : 5.596981525421143\n",
            "step : 55.51 % , loss : 5.594187259674072\n",
            "step : 55.95 % , loss : 5.646323204040527\n",
            "step : 56.39 % , loss : 5.619230270385742\n",
            "step : 56.83 % , loss : 5.4816107749938965\n",
            "step : 57.27 % , loss : 5.527449607849121\n",
            "step : 57.71 % , loss : 5.575919151306152\n",
            "step : 58.15 % , loss : 5.52169942855835\n",
            "step : 58.59 % , loss : 5.6053643226623535\n",
            "step : 59.03 % , loss : 5.606751441955566\n",
            "step : 59.47 % , loss : 5.50374698638916\n",
            "step : 59.91 % , loss : 5.543158054351807\n",
            "step : 60.35 % , loss : 5.514469623565674\n",
            "step : 60.79 % , loss : 5.569699287414551\n",
            "step : 61.23 % , loss : 5.651564598083496\n",
            "step : 61.67 % , loss : 5.45394229888916\n",
            "step : 62.11 % , loss : 5.620467662811279\n",
            "step : 62.56 % , loss : 5.70232629776001\n",
            "step : 63.0 % , loss : 5.520035743713379\n",
            "step : 63.44 % , loss : 5.610691070556641\n",
            "step : 63.88 % , loss : 5.568044662475586\n",
            "step : 64.32 % , loss : 5.6681132316589355\n",
            "step : 64.76 % , loss : 5.494151592254639\n",
            "step : 65.2 % , loss : 5.536314010620117\n",
            "step : 65.64 % , loss : 5.57604455947876\n",
            "step : 66.08 % , loss : 5.601004600524902\n",
            "step : 66.52 % , loss : 5.527156829833984\n",
            "step : 66.96 % , loss : 5.548938751220703\n",
            "step : 67.4 % , loss : 5.5587639808654785\n",
            "step : 67.84 % , loss : 5.636098861694336\n",
            "step : 68.28 % , loss : 5.6006927490234375\n",
            "step : 68.72 % , loss : 5.672630786895752\n",
            "step : 69.16 % , loss : 5.589305400848389\n",
            "step : 69.6 % , loss : 5.531678199768066\n",
            "step : 70.04 % , loss : 5.55458402633667\n",
            "step : 70.48 % , loss : 5.546839237213135\n",
            "step : 70.93 % , loss : 5.527592658996582\n",
            "step : 71.37 % , loss : 5.619003772735596\n",
            "step : 71.81 % , loss : 5.49475622177124\n",
            "step : 72.25 % , loss : 5.53298807144165\n",
            "step : 72.69 % , loss : 5.466122627258301\n",
            "step : 73.13 % , loss : 5.452060222625732\n",
            "step : 73.57 % , loss : 5.60684061050415\n",
            "step : 74.01 % , loss : 5.544761657714844\n",
            "step : 74.45 % , loss : 5.532124042510986\n",
            "step : 74.89 % , loss : 5.476332187652588\n",
            "step : 75.33 % , loss : 5.528399467468262\n",
            "step : 75.77 % , loss : 5.4935455322265625\n",
            "step : 76.21 % , loss : 5.621118068695068\n",
            "step : 76.65 % , loss : 5.428645610809326\n",
            "step : 77.09 % , loss : 5.524323463439941\n",
            "step : 77.53 % , loss : 5.563833713531494\n",
            "step : 77.97 % , loss : 5.580077648162842\n",
            "step : 78.41 % , loss : 5.5437798500061035\n",
            "step : 78.85 % , loss : 5.458366394042969\n",
            "step : 79.3 % , loss : 5.563488006591797\n",
            "step : 79.74 % , loss : 5.498149871826172\n",
            "step : 80.18 % , loss : 5.61005163192749\n",
            "step : 80.62 % , loss : 5.524573802947998\n",
            "step : 81.06 % , loss : 5.426407814025879\n",
            "step : 81.5 % , loss : 5.47025728225708\n",
            "step : 81.94 % , loss : 5.530722618103027\n",
            "step : 82.38 % , loss : 5.475185394287109\n",
            "step : 82.82 % , loss : 5.529362201690674\n",
            "step : 83.26 % , loss : 5.5558295249938965\n",
            "step : 83.7 % , loss : 5.545680046081543\n",
            "step : 84.14 % , loss : 5.504586696624756\n",
            "step : 84.58 % , loss : 5.590234756469727\n",
            "step : 85.02 % , loss : 5.462558746337891\n",
            "step : 85.46 % , loss : 5.5072021484375\n",
            "step : 85.9 % , loss : 5.452104091644287\n",
            "step : 86.34 % , loss : 5.50421142578125\n",
            "step : 86.78 % , loss : 5.503947734832764\n",
            "step : 87.22 % , loss : 5.479825496673584\n",
            "step : 87.67 % , loss : 5.471688747406006\n",
            "step : 88.11 % , loss : 5.54266357421875\n",
            "step : 88.55 % , loss : 5.603030681610107\n",
            "step : 88.99 % , loss : 5.520267486572266\n",
            "step : 89.43 % , loss : 5.468486785888672\n",
            "step : 89.87 % , loss : 5.551950454711914\n",
            "step : 90.31 % , loss : 5.46587610244751\n",
            "step : 90.75 % , loss : 5.478058815002441\n",
            "step : 91.19 % , loss : 5.562588691711426\n",
            "step : 91.63 % , loss : 5.45361328125\n",
            "step : 92.07 % , loss : 5.589380264282227\n",
            "step : 92.51 % , loss : 5.521679401397705\n",
            "step : 92.95 % , loss : 5.539753437042236\n",
            "step : 93.39 % , loss : 5.460040092468262\n",
            "step : 93.83 % , loss : 5.572526454925537\n",
            "step : 94.27 % , loss : 5.4936347007751465\n",
            "step : 94.71 % , loss : 5.4878458976745605\n",
            "step : 95.15 % , loss : 5.535306453704834\n",
            "step : 95.59 % , loss : 5.529043674468994\n",
            "step : 96.04 % , loss : 5.450052738189697\n",
            "step : 96.48 % , loss : 5.510970115661621\n",
            "step : 96.92 % , loss : 5.512313365936279\n",
            "step : 97.36 % , loss : 5.483008861541748\n",
            "step : 97.8 % , loss : 5.442983627319336\n",
            "step : 98.24 % , loss : 5.546510219573975\n",
            "step : 98.68 % , loss : 5.416312217712402\n",
            "step : 99.12 % , loss : 5.435234546661377\n",
            "step : 99.56 % , loss : 5.445075035095215\n",
            "Epoch: 3 | Time: 1m 34s\n",
            "\tTrain Loss: 5.592 | Train PPL: 268.143\n",
            "\tVal Loss: 5.613 |  Val PPL: 273.918\n",
            "\tBLEU Score: 9.299\n",
            "step : 0.0 % , loss : 5.394141674041748\n",
            "step : 0.44 % , loss : 5.5247416496276855\n",
            "step : 0.88 % , loss : 5.553324222564697\n",
            "step : 1.32 % , loss : 5.524990558624268\n",
            "step : 1.76 % , loss : 5.4521098136901855\n",
            "step : 2.2 % , loss : 5.473308563232422\n",
            "step : 2.64 % , loss : 5.535508155822754\n",
            "step : 3.08 % , loss : 5.496988773345947\n",
            "step : 3.52 % , loss : 5.48513126373291\n",
            "step : 3.96 % , loss : 5.391653537750244\n",
            "step : 4.41 % , loss : 5.501213550567627\n",
            "step : 4.85 % , loss : 5.529283046722412\n",
            "step : 5.29 % , loss : 5.499090671539307\n",
            "step : 5.73 % , loss : 5.499918460845947\n",
            "step : 6.17 % , loss : 5.538178443908691\n",
            "step : 6.61 % , loss : 5.449334621429443\n",
            "step : 7.05 % , loss : 5.5150604248046875\n",
            "step : 7.49 % , loss : 5.535325050354004\n",
            "step : 7.93 % , loss : 5.4329514503479\n",
            "step : 8.37 % , loss : 5.406487464904785\n",
            "step : 8.81 % , loss : 5.482860565185547\n",
            "step : 9.25 % , loss : 5.401621341705322\n",
            "step : 9.69 % , loss : 5.457787036895752\n",
            "step : 10.13 % , loss : 5.573498249053955\n",
            "step : 10.57 % , loss : 5.505635738372803\n",
            "step : 11.01 % , loss : 5.461596488952637\n",
            "step : 11.45 % , loss : 5.438971519470215\n",
            "step : 11.89 % , loss : 5.424960136413574\n",
            "step : 12.33 % , loss : 5.386103630065918\n",
            "step : 12.78 % , loss : 5.379864692687988\n",
            "step : 13.22 % , loss : 5.558964252471924\n",
            "step : 13.66 % , loss : 5.404825210571289\n",
            "step : 14.1 % , loss : 5.5315656661987305\n",
            "step : 14.54 % , loss : 5.537137985229492\n",
            "step : 14.98 % , loss : 5.457503795623779\n",
            "step : 15.42 % , loss : 5.541106224060059\n",
            "step : 15.86 % , loss : 5.448204040527344\n",
            "step : 16.3 % , loss : 5.408446311950684\n",
            "step : 16.74 % , loss : 5.328078269958496\n",
            "step : 17.18 % , loss : 5.522376537322998\n",
            "step : 17.62 % , loss : 5.440835952758789\n",
            "step : 18.06 % , loss : 5.398092746734619\n",
            "step : 18.5 % , loss : 5.464141368865967\n",
            "step : 18.94 % , loss : 5.44800329208374\n",
            "step : 19.38 % , loss : 5.484544277191162\n",
            "step : 19.82 % , loss : 5.487800121307373\n",
            "step : 20.26 % , loss : 5.521158695220947\n",
            "step : 20.7 % , loss : 5.468271732330322\n",
            "step : 21.15 % , loss : 5.5740580558776855\n",
            "step : 21.59 % , loss : 5.40036678314209\n",
            "step : 22.03 % , loss : 5.359640121459961\n",
            "step : 22.47 % , loss : 5.512128829956055\n",
            "step : 22.91 % , loss : 5.351072788238525\n",
            "step : 23.35 % , loss : 5.456655979156494\n",
            "step : 23.79 % , loss : 5.4239959716796875\n",
            "step : 24.23 % , loss : 5.402838230133057\n",
            "step : 24.67 % , loss : 5.400727272033691\n",
            "step : 25.11 % , loss : 5.527093410491943\n",
            "step : 25.55 % , loss : 5.423399448394775\n",
            "step : 25.99 % , loss : 5.424632549285889\n",
            "step : 26.43 % , loss : 5.387411117553711\n",
            "step : 26.87 % , loss : 5.409903049468994\n",
            "step : 27.31 % , loss : 5.383525848388672\n",
            "step : 27.75 % , loss : 5.414947986602783\n",
            "step : 28.19 % , loss : 5.457785129547119\n",
            "step : 28.63 % , loss : 5.501713275909424\n",
            "step : 29.07 % , loss : 5.392509460449219\n",
            "step : 29.52 % , loss : 5.500666618347168\n",
            "step : 29.96 % , loss : 5.371155738830566\n",
            "step : 30.4 % , loss : 5.494562149047852\n",
            "step : 30.84 % , loss : 5.404380798339844\n",
            "step : 31.28 % , loss : 5.50297212600708\n",
            "step : 31.72 % , loss : 5.367321968078613\n",
            "step : 32.16 % , loss : 5.441867828369141\n",
            "step : 32.6 % , loss : 5.481527328491211\n",
            "step : 33.04 % , loss : 5.386293888092041\n",
            "step : 33.48 % , loss : 5.428523063659668\n",
            "step : 33.92 % , loss : 5.506289005279541\n",
            "step : 34.36 % , loss : 5.450538635253906\n",
            "step : 34.8 % , loss : 5.486046314239502\n",
            "step : 35.24 % , loss : 5.412373065948486\n",
            "step : 35.68 % , loss : 5.49852180480957\n",
            "step : 36.12 % , loss : 5.513131141662598\n",
            "step : 36.56 % , loss : 5.4612555503845215\n",
            "step : 37.0 % , loss : 5.310796737670898\n",
            "step : 37.44 % , loss : 5.423829078674316\n",
            "step : 37.89 % , loss : 5.36937952041626\n",
            "step : 38.33 % , loss : 5.42301607131958\n",
            "step : 38.77 % , loss : 5.327789783477783\n",
            "step : 39.21 % , loss : 5.416484832763672\n",
            "step : 39.65 % , loss : 5.428035259246826\n",
            "step : 40.09 % , loss : 5.375217914581299\n",
            "step : 40.53 % , loss : 5.409389495849609\n",
            "step : 40.97 % , loss : 5.3445611000061035\n",
            "step : 41.41 % , loss : 5.397918224334717\n",
            "step : 41.85 % , loss : 5.4305925369262695\n",
            "step : 42.29 % , loss : 5.39288330078125\n",
            "step : 42.73 % , loss : 5.329327583312988\n",
            "step : 43.17 % , loss : 5.445283889770508\n",
            "step : 43.61 % , loss : 5.411600589752197\n",
            "step : 44.05 % , loss : 5.467789649963379\n",
            "step : 44.49 % , loss : 5.375704288482666\n",
            "step : 44.93 % , loss : 5.314560413360596\n",
            "step : 45.37 % , loss : 5.489436626434326\n",
            "step : 45.81 % , loss : 5.411694526672363\n",
            "step : 46.26 % , loss : 5.373800277709961\n",
            "step : 46.7 % , loss : 5.424951076507568\n",
            "step : 47.14 % , loss : 5.437892913818359\n",
            "step : 47.58 % , loss : 5.44793701171875\n",
            "step : 48.02 % , loss : 5.366573333740234\n",
            "step : 48.46 % , loss : 5.429534912109375\n",
            "step : 48.9 % , loss : 5.429930210113525\n",
            "step : 49.34 % , loss : 5.331122875213623\n",
            "step : 49.78 % , loss : 5.433233737945557\n",
            "step : 50.22 % , loss : 5.383943557739258\n",
            "step : 50.66 % , loss : 5.358992576599121\n",
            "step : 51.1 % , loss : 5.4436540603637695\n",
            "step : 51.54 % , loss : 5.382765293121338\n",
            "step : 51.98 % , loss : 5.457091808319092\n",
            "step : 52.42 % , loss : 5.356491565704346\n",
            "step : 52.86 % , loss : 5.298312187194824\n",
            "step : 53.3 % , loss : 5.474460601806641\n",
            "step : 53.74 % , loss : 5.337638854980469\n",
            "step : 54.19 % , loss : 5.353152751922607\n",
            "step : 54.63 % , loss : 5.44019079208374\n",
            "step : 55.07 % , loss : 5.436272621154785\n",
            "step : 55.51 % , loss : 5.328862190246582\n",
            "step : 55.95 % , loss : 5.407680511474609\n",
            "step : 56.39 % , loss : 5.321108341217041\n",
            "step : 56.83 % , loss : 5.324593544006348\n",
            "step : 57.27 % , loss : 5.3050079345703125\n",
            "step : 57.71 % , loss : 5.484408378601074\n",
            "step : 58.15 % , loss : 5.331852436065674\n",
            "step : 58.59 % , loss : 5.28811502456665\n",
            "step : 59.03 % , loss : 5.419310569763184\n",
            "step : 59.47 % , loss : 5.425426006317139\n",
            "step : 59.91 % , loss : 5.4342474937438965\n",
            "step : 60.35 % , loss : 5.302972793579102\n",
            "step : 60.79 % , loss : 5.4173431396484375\n",
            "step : 61.23 % , loss : 5.332308769226074\n",
            "step : 61.67 % , loss : 5.355822563171387\n",
            "step : 62.11 % , loss : 5.435537338256836\n",
            "step : 62.56 % , loss : 5.288071632385254\n",
            "step : 63.0 % , loss : 5.19838285446167\n",
            "step : 63.44 % , loss : 5.446488857269287\n",
            "step : 63.88 % , loss : 5.436734676361084\n",
            "step : 64.32 % , loss : 5.364163875579834\n",
            "step : 64.76 % , loss : 5.318650245666504\n",
            "step : 65.2 % , loss : 5.380376815795898\n",
            "step : 65.64 % , loss : 5.308779239654541\n",
            "step : 66.08 % , loss : 5.45135498046875\n",
            "step : 66.52 % , loss : 5.394277572631836\n",
            "step : 66.96 % , loss : 5.408119201660156\n",
            "step : 67.4 % , loss : 5.408514499664307\n",
            "step : 67.84 % , loss : 5.404867649078369\n",
            "step : 68.28 % , loss : 5.488648891448975\n",
            "step : 68.72 % , loss : 5.34669828414917\n",
            "step : 69.16 % , loss : 5.351963043212891\n",
            "step : 69.6 % , loss : 5.251232624053955\n",
            "step : 70.04 % , loss : 5.3562116622924805\n",
            "step : 70.48 % , loss : 5.411163806915283\n",
            "step : 70.93 % , loss : 5.335583686828613\n",
            "step : 71.37 % , loss : 5.343399524688721\n",
            "step : 71.81 % , loss : 5.4034624099731445\n",
            "step : 72.25 % , loss : 5.3342461585998535\n",
            "step : 72.69 % , loss : 5.357295989990234\n",
            "step : 73.13 % , loss : 5.387719631195068\n",
            "step : 73.57 % , loss : 5.260550498962402\n",
            "step : 74.01 % , loss : 5.372880935668945\n",
            "step : 74.45 % , loss : 5.442322254180908\n",
            "step : 74.89 % , loss : 5.290780544281006\n",
            "step : 75.33 % , loss : 5.4462409019470215\n",
            "step : 75.77 % , loss : 5.363717555999756\n",
            "step : 76.21 % , loss : 5.3660149574279785\n",
            "step : 76.65 % , loss : 5.392923355102539\n",
            "step : 77.09 % , loss : 5.321967601776123\n",
            "step : 77.53 % , loss : 5.528772354125977\n",
            "step : 77.97 % , loss : 5.366482734680176\n",
            "step : 78.41 % , loss : 5.323214054107666\n",
            "step : 78.85 % , loss : 5.387319564819336\n",
            "step : 79.3 % , loss : 5.311339378356934\n",
            "step : 79.74 % , loss : 5.27372932434082\n",
            "step : 80.18 % , loss : 5.407649993896484\n",
            "step : 80.62 % , loss : 5.3119893074035645\n",
            "step : 81.06 % , loss : 5.370505332946777\n",
            "step : 81.5 % , loss : 5.361073017120361\n",
            "step : 81.94 % , loss : 5.3336639404296875\n",
            "step : 82.38 % , loss : 5.407227516174316\n",
            "step : 82.82 % , loss : 5.376813888549805\n",
            "step : 83.26 % , loss : 5.391693592071533\n",
            "step : 83.7 % , loss : 5.356542587280273\n",
            "step : 84.14 % , loss : 5.281442642211914\n",
            "step : 84.58 % , loss : 5.257212162017822\n",
            "step : 85.02 % , loss : 5.417718887329102\n",
            "step : 85.46 % , loss : 5.40725564956665\n",
            "step : 85.9 % , loss : 5.425983905792236\n",
            "step : 86.34 % , loss : 5.370914459228516\n",
            "step : 86.78 % , loss : 5.342679023742676\n",
            "step : 87.22 % , loss : 5.300405979156494\n",
            "step : 87.67 % , loss : 5.405117511749268\n",
            "step : 88.11 % , loss : 5.37102746963501\n",
            "step : 88.55 % , loss : 5.304204940795898\n",
            "step : 88.99 % , loss : 5.352411270141602\n",
            "step : 89.43 % , loss : 5.369330406188965\n",
            "step : 89.87 % , loss : 5.29883337020874\n",
            "step : 90.31 % , loss : 5.328721523284912\n",
            "step : 90.75 % , loss : 5.269627094268799\n",
            "step : 91.19 % , loss : 5.319938659667969\n",
            "step : 91.63 % , loss : 5.349293231964111\n",
            "step : 92.07 % , loss : 5.368779182434082\n",
            "step : 92.51 % , loss : 5.32107400894165\n",
            "step : 92.95 % , loss : 5.410915851593018\n",
            "step : 93.39 % , loss : 5.334589958190918\n",
            "step : 93.83 % , loss : 5.334275245666504\n",
            "step : 94.27 % , loss : 5.392385005950928\n",
            "step : 94.71 % , loss : 5.257318019866943\n",
            "step : 95.15 % , loss : 5.4503350257873535\n",
            "step : 95.59 % , loss : 5.284389972686768\n",
            "step : 96.04 % , loss : 5.288293361663818\n",
            "step : 96.48 % , loss : 5.3409743309021\n",
            "step : 96.92 % , loss : 5.423172473907471\n",
            "step : 97.36 % , loss : 5.413245677947998\n",
            "step : 97.8 % , loss : 5.290740966796875\n",
            "step : 98.24 % , loss : 5.220919132232666\n",
            "step : 98.68 % , loss : 5.3508453369140625\n",
            "step : 99.12 % , loss : 5.379287242889404\n",
            "step : 99.56 % , loss : 5.377274990081787\n",
            "Epoch: 4 | Time: 1m 35s\n",
            "\tTrain Loss: 5.403 | Train PPL: 222.164\n",
            "\tVal Loss: 5.325 |  Val PPL: 205.379\n",
            "\tBLEU Score: 9.733\n",
            "step : 0.0 % , loss : 5.334036350250244\n",
            "step : 0.44 % , loss : 5.364447593688965\n",
            "step : 0.88 % , loss : 5.266680717468262\n",
            "step : 1.32 % , loss : 5.258200168609619\n",
            "step : 1.76 % , loss : 5.359943866729736\n",
            "step : 2.2 % , loss : 5.358180522918701\n",
            "step : 2.64 % , loss : 5.27368688583374\n",
            "step : 3.08 % , loss : 5.4462385177612305\n",
            "step : 3.52 % , loss : 5.220862865447998\n",
            "step : 3.96 % , loss : 5.349987983703613\n",
            "step : 4.41 % , loss : 5.29967737197876\n",
            "step : 4.85 % , loss : 5.216165542602539\n",
            "step : 5.29 % , loss : 5.281423091888428\n",
            "step : 5.73 % , loss : 5.386348247528076\n",
            "step : 6.17 % , loss : 5.318769454956055\n",
            "step : 6.61 % , loss : 5.344396591186523\n",
            "step : 7.05 % , loss : 5.267907619476318\n",
            "step : 7.49 % , loss : 5.32824182510376\n",
            "step : 7.93 % , loss : 5.295055389404297\n",
            "step : 8.37 % , loss : 5.287937641143799\n",
            "step : 8.81 % , loss : 5.330434322357178\n",
            "step : 9.25 % , loss : 5.25478982925415\n",
            "step : 9.69 % , loss : 5.237204074859619\n",
            "step : 10.13 % , loss : 5.288529396057129\n",
            "step : 10.57 % , loss : 5.333767890930176\n",
            "step : 11.01 % , loss : 5.377734184265137\n",
            "step : 11.45 % , loss : 5.319945812225342\n",
            "step : 11.89 % , loss : 5.2685627937316895\n",
            "step : 12.33 % , loss : 5.366891384124756\n",
            "step : 12.78 % , loss : 5.324801921844482\n",
            "step : 13.22 % , loss : 5.349146366119385\n",
            "step : 13.66 % , loss : 5.224091529846191\n",
            "step : 14.1 % , loss : 5.298414707183838\n",
            "step : 14.54 % , loss : 5.303895950317383\n",
            "step : 14.98 % , loss : 5.355850696563721\n",
            "step : 15.42 % , loss : 5.288295745849609\n",
            "step : 15.86 % , loss : 5.432530403137207\n",
            "step : 16.3 % , loss : 5.29673957824707\n",
            "step : 16.74 % , loss : 5.26490592956543\n",
            "step : 17.18 % , loss : 5.334506988525391\n",
            "step : 17.62 % , loss : 5.364292621612549\n",
            "step : 18.06 % , loss : 5.334089756011963\n",
            "step : 18.5 % , loss : 5.327028274536133\n",
            "step : 18.94 % , loss : 5.278669834136963\n",
            "step : 19.38 % , loss : 5.3467326164245605\n",
            "step : 19.82 % , loss : 5.389363765716553\n",
            "step : 20.26 % , loss : 5.346253395080566\n",
            "step : 20.7 % , loss : 5.280885696411133\n",
            "step : 21.15 % , loss : 5.389028072357178\n",
            "step : 21.59 % , loss : 5.246078014373779\n",
            "step : 22.03 % , loss : 5.3295111656188965\n",
            "step : 22.47 % , loss : 5.294668674468994\n",
            "step : 22.91 % , loss : 5.334899425506592\n",
            "step : 23.35 % , loss : 5.3633832931518555\n",
            "step : 23.79 % , loss : 5.2805609703063965\n",
            "step : 24.23 % , loss : 5.314617156982422\n",
            "step : 24.67 % , loss : 5.323305130004883\n",
            "step : 25.11 % , loss : 5.239978313446045\n",
            "step : 25.55 % , loss : 5.166378021240234\n",
            "step : 25.99 % , loss : 5.142450332641602\n",
            "step : 26.43 % , loss : 5.266289710998535\n",
            "step : 26.87 % , loss : 5.245024681091309\n",
            "step : 27.31 % , loss : 5.213484764099121\n",
            "step : 27.75 % , loss : 5.335934162139893\n",
            "step : 28.19 % , loss : 5.287266254425049\n",
            "step : 28.63 % , loss : 5.319336891174316\n",
            "step : 29.07 % , loss : 5.317555904388428\n",
            "step : 29.52 % , loss : 5.304203510284424\n",
            "step : 29.96 % , loss : 5.37396240234375\n",
            "step : 30.4 % , loss : 5.342015743255615\n",
            "step : 30.84 % , loss : 5.310791492462158\n",
            "step : 31.28 % , loss : 5.365497589111328\n",
            "step : 31.72 % , loss : 5.304452419281006\n",
            "step : 32.16 % , loss : 5.339049339294434\n",
            "step : 32.6 % , loss : 5.310489177703857\n",
            "step : 33.04 % , loss : 5.3023176193237305\n",
            "step : 33.48 % , loss : 5.319283962249756\n",
            "step : 33.92 % , loss : 5.207980155944824\n",
            "step : 34.36 % , loss : 5.352838039398193\n",
            "step : 34.8 % , loss : 5.298909664154053\n",
            "step : 35.24 % , loss : 5.303131580352783\n",
            "step : 35.68 % , loss : 5.367279529571533\n",
            "step : 36.12 % , loss : 5.286623477935791\n",
            "step : 36.56 % , loss : 5.300069808959961\n",
            "step : 37.0 % , loss : 5.312961101531982\n",
            "step : 37.44 % , loss : 5.315782070159912\n",
            "step : 37.89 % , loss : 5.326498985290527\n",
            "step : 38.33 % , loss : 5.375283241271973\n",
            "step : 38.77 % , loss : 5.224358558654785\n",
            "step : 39.21 % , loss : 5.274332523345947\n",
            "step : 39.65 % , loss : 5.318984508514404\n",
            "step : 40.09 % , loss : 5.276961803436279\n",
            "step : 40.53 % , loss : 5.251408576965332\n",
            "step : 40.97 % , loss : 5.2891998291015625\n",
            "step : 41.41 % , loss : 5.183341026306152\n",
            "step : 41.85 % , loss : 5.274827003479004\n",
            "step : 42.29 % , loss : 5.305280685424805\n",
            "step : 42.73 % , loss : 5.3777875900268555\n",
            "step : 43.17 % , loss : 5.3598856925964355\n",
            "step : 43.61 % , loss : 5.323122978210449\n",
            "step : 44.05 % , loss : 5.309034824371338\n",
            "step : 44.49 % , loss : 5.301419734954834\n",
            "step : 44.93 % , loss : 5.235019683837891\n",
            "step : 45.37 % , loss : 5.23951530456543\n",
            "step : 45.81 % , loss : 5.291508197784424\n",
            "step : 46.26 % , loss : 5.213566303253174\n",
            "step : 46.7 % , loss : 5.265049934387207\n",
            "step : 47.14 % , loss : 5.1988043785095215\n",
            "step : 47.58 % , loss : 5.300226211547852\n",
            "step : 48.02 % , loss : 5.2694854736328125\n",
            "step : 48.46 % , loss : 5.259826183319092\n",
            "step : 48.9 % , loss : 5.303813934326172\n",
            "step : 49.34 % , loss : 5.256080627441406\n",
            "step : 49.78 % , loss : 5.309370517730713\n",
            "step : 50.22 % , loss : 5.363483905792236\n",
            "step : 50.66 % , loss : 5.350217819213867\n",
            "step : 51.1 % , loss : 5.268068790435791\n",
            "step : 51.54 % , loss : 5.427473068237305\n",
            "step : 51.98 % , loss : 5.398796558380127\n",
            "step : 52.42 % , loss : 5.23578405380249\n",
            "step : 52.86 % , loss : 5.206733226776123\n",
            "step : 53.3 % , loss : 5.243527889251709\n",
            "step : 53.74 % , loss : 5.274504661560059\n",
            "step : 54.19 % , loss : 5.238546848297119\n",
            "step : 54.63 % , loss : 5.455667495727539\n",
            "step : 55.07 % , loss : 5.307703971862793\n",
            "step : 55.51 % , loss : 5.2691969871521\n",
            "step : 55.95 % , loss : 5.322692394256592\n",
            "step : 56.39 % , loss : 5.175897121429443\n",
            "step : 56.83 % , loss : 5.2911200523376465\n",
            "step : 57.27 % , loss : 5.239511966705322\n",
            "step : 57.71 % , loss : 5.33491849899292\n",
            "step : 58.15 % , loss : 5.368420124053955\n",
            "step : 58.59 % , loss : 5.213303565979004\n",
            "step : 59.03 % , loss : 5.306216239929199\n",
            "step : 59.47 % , loss : 5.245424270629883\n",
            "step : 59.91 % , loss : 5.305089473724365\n",
            "step : 60.35 % , loss : 5.264789581298828\n",
            "step : 60.79 % , loss : 5.208759307861328\n",
            "step : 61.23 % , loss : 5.309641361236572\n",
            "step : 61.67 % , loss : 5.316140651702881\n",
            "step : 62.11 % , loss : 5.246828556060791\n",
            "step : 62.56 % , loss : 5.312785625457764\n",
            "step : 63.0 % , loss : 5.246851921081543\n",
            "step : 63.44 % , loss : 5.2585320472717285\n",
            "step : 63.88 % , loss : 5.323903560638428\n",
            "step : 64.32 % , loss : 5.246455192565918\n",
            "step : 64.76 % , loss : 5.323929786682129\n",
            "step : 65.2 % , loss : 5.2690582275390625\n",
            "step : 65.64 % , loss : 5.223886013031006\n",
            "step : 66.08 % , loss : 5.266711235046387\n",
            "step : 66.52 % , loss : 5.316769123077393\n",
            "step : 66.96 % , loss : 5.2153143882751465\n",
            "step : 67.4 % , loss : 5.327245712280273\n",
            "step : 67.84 % , loss : 5.296008586883545\n",
            "step : 68.28 % , loss : 5.457258701324463\n",
            "step : 68.72 % , loss : 5.2539496421813965\n",
            "step : 69.16 % , loss : 5.333621501922607\n",
            "step : 69.6 % , loss : 5.1659836769104\n",
            "step : 70.04 % , loss : 5.308352470397949\n",
            "step : 70.48 % , loss : 5.206838130950928\n",
            "step : 70.93 % , loss : 5.388989448547363\n",
            "step : 71.37 % , loss : 5.320281982421875\n",
            "step : 71.81 % , loss : 5.309529781341553\n",
            "step : 72.25 % , loss : 5.334902763366699\n",
            "step : 72.69 % , loss : 5.271770477294922\n",
            "step : 73.13 % , loss : 5.282856464385986\n",
            "step : 73.57 % , loss : 5.269418239593506\n",
            "step : 74.01 % , loss : 5.291479110717773\n",
            "step : 74.45 % , loss : 5.257482528686523\n",
            "step : 74.89 % , loss : 5.320520401000977\n",
            "step : 75.33 % , loss : 5.1732916831970215\n",
            "step : 75.77 % , loss : 5.267118453979492\n",
            "step : 76.21 % , loss : 5.346034526824951\n",
            "step : 76.65 % , loss : 5.283165454864502\n",
            "step : 77.09 % , loss : 5.257320404052734\n",
            "step : 77.53 % , loss : 5.287935733795166\n",
            "step : 77.97 % , loss : 5.287202835083008\n",
            "step : 78.41 % , loss : 5.2901692390441895\n",
            "step : 78.85 % , loss : 5.2634148597717285\n",
            "step : 79.3 % , loss : 5.260893821716309\n",
            "step : 79.74 % , loss : 5.265676975250244\n",
            "step : 80.18 % , loss : 5.254973888397217\n",
            "step : 80.62 % , loss : 5.240274429321289\n",
            "step : 81.06 % , loss : 5.2299485206604\n",
            "step : 81.5 % , loss : 5.154146671295166\n",
            "step : 81.94 % , loss : 5.164881706237793\n",
            "step : 82.38 % , loss : 5.240973949432373\n",
            "step : 82.82 % , loss : 5.406955242156982\n",
            "step : 83.26 % , loss : 5.353762149810791\n",
            "step : 83.7 % , loss : 5.22710657119751\n",
            "step : 84.14 % , loss : 5.30055046081543\n",
            "step : 84.58 % , loss : 5.340087413787842\n",
            "step : 85.02 % , loss : 5.278034687042236\n",
            "step : 85.46 % , loss : 5.296188831329346\n",
            "step : 85.9 % , loss : 5.3015594482421875\n",
            "step : 86.34 % , loss : 5.132443428039551\n",
            "step : 86.78 % , loss : 5.22435188293457\n",
            "step : 87.22 % , loss : 5.1365251541137695\n",
            "step : 87.67 % , loss : 5.274123191833496\n",
            "step : 88.11 % , loss : 5.249807357788086\n",
            "step : 88.55 % , loss : 5.09126615524292\n",
            "step : 88.99 % , loss : 5.301913261413574\n",
            "step : 89.43 % , loss : 5.297321796417236\n",
            "step : 89.87 % , loss : 5.2796454429626465\n",
            "step : 90.31 % , loss : 5.214428424835205\n",
            "step : 90.75 % , loss : 5.322218418121338\n",
            "step : 91.19 % , loss : 5.143502712249756\n",
            "step : 91.63 % , loss : 5.373661518096924\n",
            "step : 92.07 % , loss : 5.253523349761963\n",
            "step : 92.51 % , loss : 5.252584457397461\n",
            "step : 92.95 % , loss : 5.219206809997559\n",
            "step : 93.39 % , loss : 5.179206848144531\n",
            "step : 93.83 % , loss : 5.2372870445251465\n",
            "step : 94.27 % , loss : 5.287282943725586\n",
            "step : 94.71 % , loss : 5.1992692947387695\n",
            "step : 95.15 % , loss : 5.248459339141846\n",
            "step : 95.59 % , loss : 5.271462440490723\n",
            "step : 96.04 % , loss : 5.312933921813965\n",
            "step : 96.48 % , loss : 5.194868564605713\n",
            "step : 96.92 % , loss : 5.199273586273193\n",
            "step : 97.36 % , loss : 5.193346977233887\n",
            "step : 97.8 % , loss : 5.273425102233887\n",
            "step : 98.24 % , loss : 5.333346843719482\n",
            "step : 98.68 % , loss : 5.355113506317139\n",
            "step : 99.12 % , loss : 5.1037492752075195\n",
            "step : 99.56 % , loss : 5.175744533538818\n",
            "Epoch: 5 | Time: 1m 34s\n",
            "\tTrain Loss: 5.287 | Train PPL: 197.802\n",
            "\tVal Loss: 5.204 |  Val PPL: 181.941\n",
            "\tBLEU Score: 12.216\n",
            "step : 0.0 % , loss : 5.225462913513184\n",
            "step : 0.44 % , loss : 5.249823093414307\n",
            "step : 0.88 % , loss : 5.242559432983398\n",
            "step : 1.32 % , loss : 5.263036727905273\n",
            "step : 1.76 % , loss : 5.304213523864746\n",
            "step : 2.2 % , loss : 5.1922831535339355\n",
            "step : 2.64 % , loss : 5.138695240020752\n",
            "step : 3.08 % , loss : 5.212372779846191\n",
            "step : 3.52 % , loss : 5.2776947021484375\n",
            "step : 3.96 % , loss : 5.2332329750061035\n",
            "step : 4.41 % , loss : 5.167160511016846\n",
            "step : 4.85 % , loss : 5.229004859924316\n",
            "step : 5.29 % , loss : 5.223385334014893\n",
            "step : 5.73 % , loss : 5.192117691040039\n",
            "step : 6.17 % , loss : 5.209237098693848\n",
            "step : 6.61 % , loss : 5.335460186004639\n",
            "step : 7.05 % , loss : 5.356261253356934\n",
            "step : 7.49 % , loss : 5.196497917175293\n",
            "step : 7.93 % , loss : 5.293458938598633\n",
            "step : 8.37 % , loss : 5.216701984405518\n",
            "step : 8.81 % , loss : 5.276078224182129\n",
            "step : 9.25 % , loss : 5.216495990753174\n",
            "step : 9.69 % , loss : 5.200753688812256\n",
            "step : 10.13 % , loss : 5.247421741485596\n",
            "step : 10.57 % , loss : 5.29309606552124\n",
            "step : 11.01 % , loss : 5.2007737159729\n",
            "step : 11.45 % , loss : 5.280356407165527\n",
            "step : 11.89 % , loss : 5.13006067276001\n",
            "step : 12.33 % , loss : 5.128407001495361\n",
            "step : 12.78 % , loss : 5.202734470367432\n",
            "step : 13.22 % , loss : 5.179790019989014\n",
            "step : 13.66 % , loss : 5.1433587074279785\n",
            "step : 14.1 % , loss : 5.255714416503906\n",
            "step : 14.54 % , loss : 5.242020130157471\n",
            "step : 14.98 % , loss : 5.310858726501465\n",
            "step : 15.42 % , loss : 5.237180233001709\n",
            "step : 15.86 % , loss : 5.354219913482666\n",
            "step : 16.3 % , loss : 5.174252033233643\n",
            "step : 16.74 % , loss : 5.321045875549316\n",
            "step : 17.18 % , loss : 5.269678592681885\n",
            "step : 17.62 % , loss : 5.114804744720459\n",
            "step : 18.06 % , loss : 5.210700511932373\n",
            "step : 18.5 % , loss : 5.205917835235596\n",
            "step : 18.94 % , loss : 5.1827545166015625\n",
            "step : 19.38 % , loss : 5.208404541015625\n",
            "step : 19.82 % , loss : 5.224355220794678\n",
            "step : 20.26 % , loss : 5.2021379470825195\n",
            "step : 20.7 % , loss : 5.306600093841553\n",
            "step : 21.15 % , loss : 5.258560657501221\n",
            "step : 21.59 % , loss : 5.128061294555664\n",
            "step : 22.03 % , loss : 5.2315192222595215\n",
            "step : 22.47 % , loss : 5.196203231811523\n",
            "step : 22.91 % , loss : 5.25583553314209\n",
            "step : 23.35 % , loss : 5.231137275695801\n",
            "step : 23.79 % , loss : 5.339898109436035\n",
            "step : 24.23 % , loss : 5.400131702423096\n",
            "step : 24.67 % , loss : 5.273251533508301\n",
            "step : 25.11 % , loss : 5.160050392150879\n",
            "step : 25.55 % , loss : 5.299692630767822\n",
            "step : 25.99 % , loss : 5.2577924728393555\n",
            "step : 26.43 % , loss : 5.2319207191467285\n",
            "step : 26.87 % , loss : 5.327579975128174\n",
            "step : 27.31 % , loss : 5.346593856811523\n",
            "step : 27.75 % , loss : 5.250913619995117\n",
            "step : 28.19 % , loss : 5.220810890197754\n",
            "step : 28.63 % , loss : 5.34974479675293\n",
            "step : 29.07 % , loss : 5.171004772186279\n",
            "step : 29.52 % , loss : 5.2602386474609375\n",
            "step : 29.96 % , loss : 5.178427696228027\n",
            "step : 30.4 % , loss : 5.191464900970459\n",
            "step : 30.84 % , loss : 5.349686622619629\n",
            "step : 31.28 % , loss : 5.318160057067871\n",
            "step : 31.72 % , loss : 5.253652572631836\n",
            "step : 32.16 % , loss : 5.136086463928223\n",
            "step : 32.6 % , loss : 5.21938419342041\n",
            "step : 33.04 % , loss : 5.185894012451172\n",
            "step : 33.48 % , loss : 5.213785171508789\n",
            "step : 33.92 % , loss : 5.219040870666504\n",
            "step : 34.36 % , loss : 5.115615367889404\n",
            "step : 34.8 % , loss : 5.133898735046387\n",
            "step : 35.24 % , loss : 5.238185405731201\n",
            "step : 35.68 % , loss : 5.278224945068359\n",
            "step : 36.12 % , loss : 5.365543365478516\n",
            "step : 36.56 % , loss : 5.238216876983643\n",
            "step : 37.0 % , loss : 5.204035758972168\n",
            "step : 37.44 % , loss : 5.045343399047852\n",
            "step : 37.89 % , loss : 5.201564311981201\n",
            "step : 38.33 % , loss : 5.268741607666016\n",
            "step : 38.77 % , loss : 5.196563720703125\n",
            "step : 39.21 % , loss : 5.1124114990234375\n",
            "step : 39.65 % , loss : 5.241633415222168\n",
            "step : 40.09 % , loss : 5.2507853507995605\n",
            "step : 40.53 % , loss : 5.335855007171631\n",
            "step : 40.97 % , loss : 5.202306747436523\n",
            "step : 41.41 % , loss : 5.184878826141357\n",
            "step : 41.85 % , loss : 5.265163898468018\n",
            "step : 42.29 % , loss : 5.17272424697876\n",
            "step : 42.73 % , loss : 5.199465751647949\n",
            "step : 43.17 % , loss : 5.289626121520996\n",
            "step : 43.61 % , loss : 5.1953654289245605\n",
            "step : 44.05 % , loss : 5.275635242462158\n",
            "step : 44.49 % , loss : 5.271203517913818\n",
            "step : 44.93 % , loss : 5.200130462646484\n",
            "step : 45.37 % , loss : 5.158560276031494\n",
            "step : 45.81 % , loss : 5.182267665863037\n",
            "step : 46.26 % , loss : 5.162292957305908\n",
            "step : 46.7 % , loss : 5.1834797859191895\n",
            "step : 47.14 % , loss : 5.198477268218994\n",
            "step : 47.58 % , loss : 5.256749153137207\n",
            "step : 48.02 % , loss : 5.09401798248291\n",
            "step : 48.46 % , loss : 5.297306537628174\n",
            "step : 48.9 % , loss : 5.140100955963135\n",
            "step : 49.34 % , loss : 5.3420538902282715\n",
            "step : 49.78 % , loss : 5.159890651702881\n",
            "step : 50.22 % , loss : 5.290225505828857\n",
            "step : 50.66 % , loss : 5.198367118835449\n",
            "step : 51.1 % , loss : 5.2874650955200195\n",
            "step : 51.54 % , loss : 5.226696491241455\n",
            "step : 51.98 % , loss : 5.213219165802002\n",
            "step : 52.42 % , loss : 5.235835075378418\n",
            "step : 52.86 % , loss : 5.213240623474121\n",
            "step : 53.3 % , loss : 5.24846887588501\n",
            "step : 53.74 % , loss : 5.1713690757751465\n",
            "step : 54.19 % , loss : 5.176055908203125\n",
            "step : 54.63 % , loss : 5.170462608337402\n",
            "step : 55.07 % , loss : 5.254759311676025\n",
            "step : 55.51 % , loss : 5.173503398895264\n",
            "step : 55.95 % , loss : 5.095192909240723\n",
            "step : 56.39 % , loss : 5.13762092590332\n",
            "step : 56.83 % , loss : 5.218031406402588\n",
            "step : 57.27 % , loss : 5.135101795196533\n",
            "step : 57.71 % , loss : 5.137617588043213\n",
            "step : 58.15 % , loss : 5.08901834487915\n",
            "step : 58.59 % , loss : 5.182132244110107\n",
            "step : 59.03 % , loss : 5.222039699554443\n",
            "step : 59.47 % , loss : 5.198245525360107\n",
            "step : 59.91 % , loss : 5.18906307220459\n",
            "step : 60.35 % , loss : 5.196652889251709\n",
            "step : 60.79 % , loss : 5.167722225189209\n",
            "step : 61.23 % , loss : 5.238487243652344\n",
            "step : 61.67 % , loss : 5.223437309265137\n",
            "step : 62.11 % , loss : 5.278805255889893\n",
            "step : 62.56 % , loss : 5.296032905578613\n",
            "step : 63.0 % , loss : 5.216285705566406\n",
            "step : 63.44 % , loss : 5.18815803527832\n",
            "step : 63.88 % , loss : 5.200799942016602\n",
            "step : 64.32 % , loss : 5.146246433258057\n",
            "step : 64.76 % , loss : 5.275120258331299\n",
            "step : 65.2 % , loss : 5.2535929679870605\n",
            "step : 65.64 % , loss : 5.143265247344971\n",
            "step : 66.08 % , loss : 5.182130813598633\n",
            "step : 66.52 % , loss : 5.164393424987793\n",
            "step : 66.96 % , loss : 5.244261741638184\n",
            "step : 67.4 % , loss : 5.171098232269287\n",
            "step : 67.84 % , loss : 5.284494876861572\n",
            "step : 68.28 % , loss : 5.283837795257568\n",
            "step : 68.72 % , loss : 5.238981246948242\n",
            "step : 69.16 % , loss : 5.144352436065674\n",
            "step : 69.6 % , loss : 5.199908256530762\n",
            "step : 70.04 % , loss : 5.164543628692627\n",
            "step : 70.48 % , loss : 5.201637268066406\n",
            "step : 70.93 % , loss : 5.198297023773193\n",
            "step : 71.37 % , loss : 5.157832145690918\n",
            "step : 71.81 % , loss : 5.1456193923950195\n",
            "step : 72.25 % , loss : 5.123006343841553\n",
            "step : 72.69 % , loss : 5.073257923126221\n",
            "step : 73.13 % , loss : 5.094604015350342\n",
            "step : 73.57 % , loss : 5.1638031005859375\n",
            "step : 74.01 % , loss : 5.266018867492676\n",
            "step : 74.45 % , loss : 5.204967498779297\n",
            "step : 74.89 % , loss : 5.23594856262207\n",
            "step : 75.33 % , loss : 5.256665229797363\n",
            "step : 75.77 % , loss : 5.165696620941162\n",
            "step : 76.21 % , loss : 5.213393211364746\n",
            "step : 76.65 % , loss : 5.224241733551025\n",
            "step : 77.09 % , loss : 5.142404079437256\n",
            "step : 77.53 % , loss : 5.196693420410156\n",
            "step : 77.97 % , loss : 5.230669975280762\n",
            "step : 78.41 % , loss : 5.133202075958252\n",
            "step : 78.85 % , loss : 5.121675491333008\n",
            "step : 79.3 % , loss : 5.22055721282959\n",
            "step : 79.74 % , loss : 5.241784572601318\n",
            "step : 80.18 % , loss : 5.0843329429626465\n",
            "step : 80.62 % , loss : 5.202567100524902\n",
            "step : 81.06 % , loss : 5.187740325927734\n",
            "step : 81.5 % , loss : 5.182183742523193\n",
            "step : 81.94 % , loss : 5.174781322479248\n",
            "step : 82.38 % , loss : 5.194190979003906\n",
            "step : 82.82 % , loss : 5.248237133026123\n",
            "step : 83.26 % , loss : 5.144924640655518\n",
            "step : 83.7 % , loss : 5.143601894378662\n",
            "step : 84.14 % , loss : 5.176938533782959\n",
            "step : 84.58 % , loss : 5.134055137634277\n",
            "step : 85.02 % , loss : 5.283453941345215\n",
            "step : 85.46 % , loss : 5.149107456207275\n",
            "step : 85.9 % , loss : 5.079153537750244\n",
            "step : 86.34 % , loss : 5.22671365737915\n",
            "step : 86.78 % , loss : 5.203134059906006\n",
            "step : 87.22 % , loss : 5.120355129241943\n",
            "step : 87.67 % , loss : 5.252018928527832\n",
            "step : 88.11 % , loss : 5.285750865936279\n",
            "step : 88.55 % , loss : 5.17820405960083\n",
            "step : 88.99 % , loss : 5.225011348724365\n",
            "step : 89.43 % , loss : 5.218643665313721\n",
            "step : 89.87 % , loss : 5.199688911437988\n",
            "step : 90.31 % , loss : 5.240294933319092\n",
            "step : 90.75 % , loss : 5.1685075759887695\n",
            "step : 91.19 % , loss : 5.111749172210693\n",
            "step : 91.63 % , loss : 5.2216973304748535\n",
            "step : 92.07 % , loss : 5.121382713317871\n",
            "step : 92.51 % , loss : 5.185014247894287\n",
            "step : 92.95 % , loss : 5.267435073852539\n",
            "step : 93.39 % , loss : 5.223278999328613\n",
            "step : 93.83 % , loss : 5.271258354187012\n",
            "step : 94.27 % , loss : 5.164434432983398\n",
            "step : 94.71 % , loss : 5.253042697906494\n",
            "step : 95.15 % , loss : 5.149667739868164\n",
            "step : 95.59 % , loss : 5.336306571960449\n",
            "step : 96.04 % , loss : 5.106362342834473\n",
            "step : 96.48 % , loss : 5.22471809387207\n",
            "step : 96.92 % , loss : 5.1637420654296875\n",
            "step : 97.36 % , loss : 5.078314304351807\n",
            "step : 97.8 % , loss : 5.134650230407715\n",
            "step : 98.24 % , loss : 5.172258377075195\n",
            "step : 98.68 % , loss : 5.222234725952148\n",
            "step : 99.12 % , loss : 5.1849470138549805\n",
            "step : 99.56 % , loss : 5.182694911956787\n",
            "Epoch: 6 | Time: 1m 34s\n",
            "\tTrain Loss: 5.212 | Train PPL: 183.401\n",
            "\tVal Loss: 5.049 |  Val PPL: 155.868\n",
            "\tBLEU Score: 11.312\n",
            "step : 0.0 % , loss : 5.234838962554932\n",
            "step : 0.44 % , loss : 5.1161417961120605\n",
            "step : 0.88 % , loss : 5.12748908996582\n",
            "step : 1.32 % , loss : 5.209986209869385\n",
            "step : 1.76 % , loss : 5.252439498901367\n",
            "step : 2.2 % , loss : 5.279935836791992\n",
            "step : 2.64 % , loss : 5.185362339019775\n",
            "step : 3.08 % , loss : 5.086165904998779\n",
            "step : 3.52 % , loss : 5.119100093841553\n",
            "step : 3.96 % , loss : 5.197554111480713\n",
            "step : 4.41 % , loss : 5.208248615264893\n",
            "step : 4.85 % , loss : 5.140373706817627\n",
            "step : 5.29 % , loss : 5.1901021003723145\n",
            "step : 5.73 % , loss : 5.1849446296691895\n",
            "step : 6.17 % , loss : 5.220983505249023\n",
            "step : 6.61 % , loss : 5.269290924072266\n",
            "step : 7.05 % , loss : 5.164108753204346\n",
            "step : 7.49 % , loss : 5.262323379516602\n",
            "step : 7.93 % , loss : 5.208861827850342\n",
            "step : 8.37 % , loss : 5.270918369293213\n",
            "step : 8.81 % , loss : 5.201742649078369\n",
            "step : 9.25 % , loss : 5.200430870056152\n",
            "step : 9.69 % , loss : 5.232625961303711\n",
            "step : 10.13 % , loss : 5.026081562042236\n",
            "step : 10.57 % , loss : 5.141717910766602\n",
            "step : 11.01 % , loss : 5.196787357330322\n",
            "step : 11.45 % , loss : 5.29426908493042\n",
            "step : 11.89 % , loss : 5.207029819488525\n",
            "step : 12.33 % , loss : 5.124728202819824\n",
            "step : 12.78 % , loss : 5.020380020141602\n",
            "step : 13.22 % , loss : 5.290713310241699\n",
            "step : 13.66 % , loss : 5.17734432220459\n",
            "step : 14.1 % , loss : 5.1392130851745605\n",
            "step : 14.54 % , loss : 5.139041900634766\n",
            "step : 14.98 % , loss : 5.243821144104004\n",
            "step : 15.42 % , loss : 5.177298069000244\n",
            "step : 15.86 % , loss : 5.122622966766357\n",
            "step : 16.3 % , loss : 5.1158318519592285\n",
            "step : 16.74 % , loss : 5.140657424926758\n",
            "step : 17.18 % , loss : 5.183760166168213\n",
            "step : 17.62 % , loss : 5.0746331214904785\n",
            "step : 18.06 % , loss : 5.162830352783203\n",
            "step : 18.5 % , loss : 5.20352840423584\n",
            "step : 18.94 % , loss : 5.210331916809082\n",
            "step : 19.38 % , loss : 5.175605773925781\n",
            "step : 19.82 % , loss : 5.080312728881836\n",
            "step : 20.26 % , loss : 5.159423828125\n",
            "step : 20.7 % , loss : 5.010697364807129\n",
            "step : 21.15 % , loss : 5.194515705108643\n",
            "step : 21.59 % , loss : 5.139144420623779\n",
            "step : 22.03 % , loss : 5.0649847984313965\n",
            "step : 22.47 % , loss : 5.231740474700928\n",
            "step : 22.91 % , loss : 5.202090263366699\n",
            "step : 23.35 % , loss : 5.259971618652344\n",
            "step : 23.79 % , loss : 5.237374782562256\n",
            "step : 24.23 % , loss : 5.170202255249023\n",
            "step : 24.67 % , loss : 5.140770435333252\n",
            "step : 25.11 % , loss : 5.040529251098633\n",
            "step : 25.55 % , loss : 5.189432621002197\n",
            "step : 25.99 % , loss : 5.143390655517578\n",
            "step : 26.43 % , loss : 5.110776424407959\n",
            "step : 26.87 % , loss : 5.235344409942627\n",
            "step : 27.31 % , loss : 5.184901714324951\n",
            "step : 27.75 % , loss : 5.1506242752075195\n",
            "step : 28.19 % , loss : 5.146284580230713\n",
            "step : 28.63 % , loss : 5.153626918792725\n",
            "step : 29.07 % , loss : 5.286315441131592\n",
            "step : 29.52 % , loss : 5.234574794769287\n",
            "step : 29.96 % , loss : 5.1281938552856445\n",
            "step : 30.4 % , loss : 5.197081089019775\n",
            "step : 30.84 % , loss : 5.178676128387451\n",
            "step : 31.28 % , loss : 5.232692241668701\n",
            "step : 31.72 % , loss : 5.141179084777832\n",
            "step : 32.16 % , loss : 5.215868949890137\n",
            "step : 32.6 % , loss : 5.1692214012146\n",
            "step : 33.04 % , loss : 5.2036662101745605\n",
            "step : 33.48 % , loss : 5.251452445983887\n",
            "step : 33.92 % , loss : 5.113089561462402\n",
            "step : 34.36 % , loss : 5.1743855476379395\n",
            "step : 34.8 % , loss : 5.174097537994385\n",
            "step : 35.24 % , loss : 5.142151355743408\n",
            "step : 35.68 % , loss : 5.158298492431641\n",
            "step : 36.12 % , loss : 5.116114139556885\n",
            "step : 36.56 % , loss : 5.250168800354004\n",
            "step : 37.0 % , loss : 5.084572792053223\n",
            "step : 37.44 % , loss : 5.211947917938232\n",
            "step : 37.89 % , loss : 5.135693550109863\n",
            "step : 38.33 % , loss : 5.228786468505859\n",
            "step : 38.77 % , loss : 5.161079406738281\n",
            "step : 39.21 % , loss : 5.310568809509277\n",
            "step : 39.65 % , loss : 5.183624267578125\n",
            "step : 40.09 % , loss : 5.138484477996826\n",
            "step : 40.53 % , loss : 5.030897617340088\n",
            "step : 40.97 % , loss : 5.106942653656006\n",
            "step : 41.41 % , loss : 5.069228649139404\n",
            "step : 41.85 % , loss : 5.286139011383057\n",
            "step : 42.29 % , loss : 5.1229329109191895\n",
            "step : 42.73 % , loss : 5.135745525360107\n",
            "step : 43.17 % , loss : 5.1362175941467285\n",
            "step : 43.61 % , loss : 5.212556838989258\n",
            "step : 44.05 % , loss : 5.156107425689697\n",
            "step : 44.49 % , loss : 5.139410018920898\n",
            "step : 44.93 % , loss : 5.097456932067871\n",
            "step : 45.37 % , loss : 5.177901268005371\n",
            "step : 45.81 % , loss : 5.117632865905762\n",
            "step : 46.26 % , loss : 5.197735786437988\n",
            "step : 46.7 % , loss : 5.318308353424072\n",
            "step : 47.14 % , loss : 5.2595648765563965\n",
            "step : 47.58 % , loss : 5.187037944793701\n",
            "step : 48.02 % , loss : 5.11080265045166\n",
            "step : 48.46 % , loss : 5.190363883972168\n",
            "step : 48.9 % , loss : 5.213186740875244\n",
            "step : 49.34 % , loss : 5.110684394836426\n",
            "step : 49.78 % , loss : 5.132267475128174\n",
            "step : 50.22 % , loss : 5.131031036376953\n",
            "step : 50.66 % , loss : 5.279449462890625\n",
            "step : 51.1 % , loss : 5.262451648712158\n",
            "step : 51.54 % , loss : 5.192026615142822\n",
            "step : 51.98 % , loss : 5.181241512298584\n",
            "step : 52.42 % , loss : 5.160017967224121\n",
            "step : 52.86 % , loss : 5.177948951721191\n",
            "step : 53.3 % , loss : 5.116903781890869\n",
            "step : 53.74 % , loss : 5.156910419464111\n",
            "step : 54.19 % , loss : 5.192400932312012\n",
            "step : 54.63 % , loss : 5.181217193603516\n",
            "step : 55.07 % , loss : 5.153135776519775\n",
            "step : 55.51 % , loss : 5.151671886444092\n",
            "step : 55.95 % , loss : 5.189550876617432\n",
            "step : 56.39 % , loss : 5.144262313842773\n",
            "step : 56.83 % , loss : 5.168201923370361\n",
            "step : 57.27 % , loss : 5.2034912109375\n",
            "step : 57.71 % , loss : 5.216914653778076\n",
            "step : 58.15 % , loss : 5.208301067352295\n",
            "step : 58.59 % , loss : 5.10697603225708\n",
            "step : 59.03 % , loss : 5.188201904296875\n",
            "step : 59.47 % , loss : 5.025041580200195\n",
            "step : 59.91 % , loss : 5.1982903480529785\n",
            "step : 60.35 % , loss : 5.169681549072266\n",
            "step : 60.79 % , loss : 5.151144504547119\n",
            "step : 61.23 % , loss : 5.189919471740723\n",
            "step : 61.67 % , loss : 5.22918176651001\n",
            "step : 62.11 % , loss : 5.146668910980225\n",
            "step : 62.56 % , loss : 5.080953598022461\n",
            "step : 63.0 % , loss : 5.0382184982299805\n",
            "step : 63.44 % , loss : 5.188358306884766\n",
            "step : 63.88 % , loss : 5.14773416519165\n",
            "step : 64.32 % , loss : 5.138075351715088\n",
            "step : 64.76 % , loss : 5.142067909240723\n",
            "step : 65.2 % , loss : 5.0708794593811035\n",
            "step : 65.64 % , loss : 5.188511848449707\n",
            "step : 66.08 % , loss : 5.025628089904785\n",
            "step : 66.52 % , loss : 5.1164398193359375\n",
            "step : 66.96 % , loss : 5.091552257537842\n",
            "step : 67.4 % , loss : 5.21762228012085\n",
            "step : 67.84 % , loss : 5.141128063201904\n",
            "step : 68.28 % , loss : 5.054615020751953\n",
            "step : 68.72 % , loss : 5.175387859344482\n",
            "step : 69.16 % , loss : 5.1222920417785645\n",
            "step : 69.6 % , loss : 5.239058971405029\n",
            "step : 70.04 % , loss : 5.176393985748291\n",
            "step : 70.48 % , loss : 5.130664348602295\n",
            "step : 70.93 % , loss : 5.163578510284424\n",
            "step : 71.37 % , loss : 5.118648529052734\n",
            "step : 71.81 % , loss : 5.1813249588012695\n",
            "step : 72.25 % , loss : 5.177145957946777\n",
            "step : 72.69 % , loss : 5.23932409286499\n",
            "step : 73.13 % , loss : 5.146320819854736\n",
            "step : 73.57 % , loss : 5.137426376342773\n",
            "step : 74.01 % , loss : 5.099050998687744\n",
            "step : 74.45 % , loss : 5.211063861846924\n",
            "step : 74.89 % , loss : 5.129933834075928\n",
            "step : 75.33 % , loss : 5.195094585418701\n",
            "step : 75.77 % , loss : 5.111276626586914\n",
            "step : 76.21 % , loss : 5.185610294342041\n",
            "step : 76.65 % , loss : 5.2087202072143555\n",
            "step : 77.09 % , loss : 5.206618309020996\n",
            "step : 77.53 % , loss : 5.263423919677734\n",
            "step : 77.97 % , loss : 5.186453342437744\n",
            "step : 78.41 % , loss : 5.100647449493408\n",
            "step : 78.85 % , loss : 5.157699108123779\n",
            "step : 79.3 % , loss : 5.138004779815674\n",
            "step : 79.74 % , loss : 5.137498378753662\n",
            "step : 80.18 % , loss : 5.148885250091553\n",
            "step : 80.62 % , loss : 5.188465595245361\n",
            "step : 81.06 % , loss : 5.158457279205322\n",
            "step : 81.5 % , loss : 5.1310505867004395\n",
            "step : 81.94 % , loss : 5.190534591674805\n",
            "step : 82.38 % , loss : 5.100791931152344\n",
            "step : 82.82 % , loss : 5.149858474731445\n",
            "step : 83.26 % , loss : 5.121127605438232\n",
            "step : 83.7 % , loss : 5.2102460861206055\n",
            "step : 84.14 % , loss : 5.037436485290527\n",
            "step : 84.58 % , loss : 5.161314487457275\n",
            "step : 85.02 % , loss : 5.101402759552002\n",
            "step : 85.46 % , loss : 5.0509748458862305\n",
            "step : 85.9 % , loss : 5.04646110534668\n",
            "step : 86.34 % , loss : 5.095041751861572\n",
            "step : 86.78 % , loss : 5.220337867736816\n",
            "step : 87.22 % , loss : 5.101290225982666\n",
            "step : 87.67 % , loss : 5.062463283538818\n",
            "step : 88.11 % , loss : 5.111520767211914\n",
            "step : 88.55 % , loss : 5.176138877868652\n",
            "step : 88.99 % , loss : 5.114984512329102\n",
            "step : 89.43 % , loss : 5.094158172607422\n",
            "step : 89.87 % , loss : 5.114787578582764\n",
            "step : 90.31 % , loss : 5.161342144012451\n",
            "step : 90.75 % , loss : 5.109236717224121\n",
            "step : 91.19 % , loss : 5.055559158325195\n",
            "step : 91.63 % , loss : 5.238839149475098\n",
            "step : 92.07 % , loss : 5.142024040222168\n",
            "step : 92.51 % , loss : 5.179226875305176\n",
            "step : 92.95 % , loss : 5.134854316711426\n",
            "step : 93.39 % , loss : 5.018904209136963\n",
            "step : 93.83 % , loss : 5.094627857208252\n",
            "step : 94.27 % , loss : 5.044269561767578\n",
            "step : 94.71 % , loss : 5.150183200836182\n",
            "step : 95.15 % , loss : 5.04063081741333\n",
            "step : 95.59 % , loss : 5.248590469360352\n",
            "step : 96.04 % , loss : 5.016259670257568\n",
            "step : 96.48 % , loss : 5.203527927398682\n",
            "step : 96.92 % , loss : 5.093912601470947\n",
            "step : 97.36 % , loss : 5.124660968780518\n",
            "step : 97.8 % , loss : 5.219268321990967\n",
            "step : 98.24 % , loss : 5.130177021026611\n",
            "step : 98.68 % , loss : 5.2256927490234375\n",
            "step : 99.12 % , loss : 5.1478471755981445\n",
            "step : 99.56 % , loss : 5.154120445251465\n",
            "Epoch: 7 | Time: 1m 34s\n",
            "\tTrain Loss: 5.160 | Train PPL: 174.213\n",
            "\tVal Loss: 5.032 |  Val PPL: 153.224\n",
            "\tBLEU Score: 10.702\n",
            "step : 0.0 % , loss : 5.228548049926758\n",
            "step : 0.44 % , loss : 5.160938262939453\n",
            "step : 0.88 % , loss : 5.04716682434082\n",
            "step : 1.32 % , loss : 5.103036403656006\n",
            "step : 1.76 % , loss : 5.073116302490234\n",
            "step : 2.2 % , loss : 5.151401519775391\n",
            "step : 2.64 % , loss : 5.268787384033203\n",
            "step : 3.08 % , loss : 5.209934234619141\n",
            "step : 3.52 % , loss : 5.169994354248047\n",
            "step : 3.96 % , loss : 5.1413702964782715\n",
            "step : 4.41 % , loss : 5.166314601898193\n",
            "step : 4.85 % , loss : 5.106977939605713\n",
            "step : 5.29 % , loss : 5.160050868988037\n",
            "step : 5.73 % , loss : 5.180047988891602\n",
            "step : 6.17 % , loss : 5.07852029800415\n",
            "step : 6.61 % , loss : 5.1999688148498535\n",
            "step : 7.05 % , loss : 5.301052093505859\n",
            "step : 7.49 % , loss : 5.107551097869873\n",
            "step : 7.93 % , loss : 5.205683708190918\n",
            "step : 8.37 % , loss : 5.070876598358154\n",
            "step : 8.81 % , loss : 5.0765790939331055\n",
            "step : 9.25 % , loss : 5.160928726196289\n",
            "step : 9.69 % , loss : 5.025844097137451\n",
            "step : 10.13 % , loss : 5.053701877593994\n",
            "step : 10.57 % , loss : 5.17238712310791\n",
            "step : 11.01 % , loss : 5.074121475219727\n",
            "step : 11.45 % , loss : 5.1434431076049805\n",
            "step : 11.89 % , loss : 5.119473934173584\n",
            "step : 12.33 % , loss : 5.160079002380371\n",
            "step : 12.78 % , loss : 5.1700873374938965\n",
            "step : 13.22 % , loss : 5.130301475524902\n",
            "step : 13.66 % , loss : 5.237028121948242\n",
            "step : 14.1 % , loss : 5.076943874359131\n",
            "step : 14.54 % , loss : 5.100873947143555\n",
            "step : 14.98 % , loss : 5.1416192054748535\n",
            "step : 15.42 % , loss : 5.178952217102051\n",
            "step : 15.86 % , loss : 5.165366172790527\n",
            "step : 16.3 % , loss : 5.189785957336426\n",
            "step : 16.74 % , loss : 5.109947204589844\n",
            "step : 17.18 % , loss : 5.185331344604492\n",
            "step : 17.62 % , loss : 4.993500709533691\n",
            "step : 18.06 % , loss : 5.056018352508545\n",
            "step : 18.5 % , loss : 5.103596210479736\n",
            "step : 18.94 % , loss : 5.174972057342529\n",
            "step : 19.38 % , loss : 5.1752495765686035\n",
            "step : 19.82 % , loss : 5.130285739898682\n",
            "step : 20.26 % , loss : 5.0632805824279785\n",
            "step : 20.7 % , loss : 5.075677394866943\n",
            "step : 21.15 % , loss : 5.08887243270874\n",
            "step : 21.59 % , loss : 5.188666343688965\n",
            "step : 22.03 % , loss : 5.154024600982666\n",
            "step : 22.47 % , loss : 5.129028797149658\n",
            "step : 22.91 % , loss : 5.19828462600708\n",
            "step : 23.35 % , loss : 5.116801738739014\n",
            "step : 23.79 % , loss : 5.136831283569336\n",
            "step : 24.23 % , loss : 5.052529335021973\n",
            "step : 24.67 % , loss : 5.169821739196777\n",
            "step : 25.11 % , loss : 5.140694618225098\n",
            "step : 25.55 % , loss : 5.034628391265869\n",
            "step : 25.99 % , loss : 5.203062057495117\n",
            "step : 26.43 % , loss : 5.1656060218811035\n",
            "step : 26.87 % , loss : 5.082179546356201\n",
            "step : 27.31 % , loss : 5.105396270751953\n",
            "step : 27.75 % , loss : 5.182508945465088\n",
            "step : 28.19 % , loss : 5.010025978088379\n",
            "step : 28.63 % , loss : 5.198866367340088\n",
            "step : 29.07 % , loss : 5.135613441467285\n",
            "step : 29.52 % , loss : 5.122844219207764\n",
            "step : 29.96 % , loss : 5.119751453399658\n",
            "step : 30.4 % , loss : 5.228135585784912\n",
            "step : 30.84 % , loss : 5.064260005950928\n",
            "step : 31.28 % , loss : 5.128578186035156\n",
            "step : 31.72 % , loss : 5.129724025726318\n",
            "step : 32.16 % , loss : 5.136545658111572\n",
            "step : 32.6 % , loss : 5.19474983215332\n",
            "step : 33.04 % , loss : 5.208367347717285\n",
            "step : 33.48 % , loss : 5.064914226531982\n",
            "step : 33.92 % , loss : 5.241644382476807\n",
            "step : 34.36 % , loss : 5.008439064025879\n",
            "step : 34.8 % , loss : 5.155856132507324\n",
            "step : 35.24 % , loss : 5.188571929931641\n",
            "step : 35.68 % , loss : 5.078024864196777\n",
            "step : 36.12 % , loss : 5.030887126922607\n",
            "step : 36.56 % , loss : 5.189373016357422\n",
            "step : 37.0 % , loss : 5.040724277496338\n",
            "step : 37.44 % , loss : 5.166764736175537\n",
            "step : 37.89 % , loss : 5.122832775115967\n",
            "step : 38.33 % , loss : 5.054071426391602\n",
            "step : 38.77 % , loss : 5.1432204246521\n",
            "step : 39.21 % , loss : 5.1764912605285645\n",
            "step : 39.65 % , loss : 5.213419437408447\n",
            "step : 40.09 % , loss : 5.216975688934326\n",
            "step : 40.53 % , loss : 5.04061222076416\n",
            "step : 40.97 % , loss : 5.214148998260498\n",
            "step : 41.41 % , loss : 5.170622825622559\n",
            "step : 41.85 % , loss : 5.163614273071289\n",
            "step : 42.29 % , loss : 4.967144966125488\n",
            "step : 42.73 % , loss : 5.179105758666992\n",
            "step : 43.17 % , loss : 5.169957160949707\n",
            "step : 43.61 % , loss : 5.025647163391113\n",
            "step : 44.05 % , loss : 5.1371259689331055\n",
            "step : 44.49 % , loss : 5.13823127746582\n",
            "step : 44.93 % , loss : 5.0746588706970215\n",
            "step : 45.37 % , loss : 5.106894016265869\n",
            "step : 45.81 % , loss : 5.142786026000977\n",
            "step : 46.26 % , loss : 5.099010944366455\n",
            "step : 46.7 % , loss : 5.064682960510254\n",
            "step : 47.14 % , loss : 5.090935707092285\n",
            "step : 47.58 % , loss : 5.091030597686768\n",
            "step : 48.02 % , loss : 5.116090774536133\n",
            "step : 48.46 % , loss : 5.128819942474365\n",
            "step : 48.9 % , loss : 5.14000940322876\n",
            "step : 49.34 % , loss : 5.103377819061279\n",
            "step : 49.78 % , loss : 5.119431018829346\n",
            "step : 50.22 % , loss : 5.116518974304199\n",
            "step : 50.66 % , loss : 5.038276195526123\n",
            "step : 51.1 % , loss : 5.066213130950928\n",
            "step : 51.54 % , loss : 5.1046929359436035\n",
            "step : 51.98 % , loss : 5.095341205596924\n",
            "step : 52.42 % , loss : 5.042640209197998\n",
            "step : 52.86 % , loss : 5.040422439575195\n",
            "step : 53.3 % , loss : 5.054375171661377\n",
            "step : 53.74 % , loss : 5.164004325866699\n",
            "step : 54.19 % , loss : 5.0133209228515625\n",
            "step : 54.63 % , loss : 5.067816257476807\n",
            "step : 55.07 % , loss : 5.140725135803223\n",
            "step : 55.51 % , loss : 5.055216312408447\n",
            "step : 55.95 % , loss : 5.084740161895752\n",
            "step : 56.39 % , loss : 5.1537652015686035\n",
            "step : 56.83 % , loss : 5.154516220092773\n",
            "step : 57.27 % , loss : 5.22970724105835\n",
            "step : 57.71 % , loss : 5.041045188903809\n",
            "step : 58.15 % , loss : 5.126750469207764\n",
            "step : 58.59 % , loss : 5.071754455566406\n",
            "step : 59.03 % , loss : 5.1249494552612305\n",
            "step : 59.47 % , loss : 5.084676742553711\n",
            "step : 59.91 % , loss : 4.998863220214844\n",
            "step : 60.35 % , loss : 5.108420372009277\n",
            "step : 60.79 % , loss : 5.199592590332031\n",
            "step : 61.23 % , loss : 5.13249397277832\n",
            "step : 61.67 % , loss : 5.091529846191406\n",
            "step : 62.11 % , loss : 5.18975305557251\n",
            "step : 62.56 % , loss : 5.271313667297363\n",
            "step : 63.0 % , loss : 5.159605979919434\n",
            "step : 63.44 % , loss : 5.146407127380371\n",
            "step : 63.88 % , loss : 5.11789083480835\n",
            "step : 64.32 % , loss : 5.1158881187438965\n",
            "step : 64.76 % , loss : 5.0868401527404785\n",
            "step : 65.2 % , loss : 5.148993015289307\n",
            "step : 65.64 % , loss : 5.217779159545898\n",
            "step : 66.08 % , loss : 5.096811294555664\n",
            "step : 66.52 % , loss : 5.096696853637695\n",
            "step : 66.96 % , loss : 5.151137351989746\n",
            "step : 67.4 % , loss : 5.10294771194458\n",
            "step : 67.84 % , loss : 4.996350288391113\n",
            "step : 68.28 % , loss : 5.1599016189575195\n",
            "step : 68.72 % , loss : 5.145561695098877\n",
            "step : 69.16 % , loss : 5.167652130126953\n",
            "step : 69.6 % , loss : 5.044043064117432\n",
            "step : 70.04 % , loss : 5.150759220123291\n",
            "step : 70.48 % , loss : 5.13040828704834\n",
            "step : 70.93 % , loss : 5.144397735595703\n",
            "step : 71.37 % , loss : 5.123471260070801\n",
            "step : 71.81 % , loss : 5.092109203338623\n",
            "step : 72.25 % , loss : 5.093935489654541\n",
            "step : 72.69 % , loss : 5.171096324920654\n",
            "step : 73.13 % , loss : 5.109442234039307\n",
            "step : 73.57 % , loss : 5.065550804138184\n",
            "step : 74.01 % , loss : 5.125119686126709\n",
            "step : 74.45 % , loss : 5.084590911865234\n",
            "step : 74.89 % , loss : 5.096573829650879\n",
            "step : 75.33 % , loss : 4.961759090423584\n",
            "step : 75.77 % , loss : 5.211972236633301\n",
            "step : 76.21 % , loss : 5.187463283538818\n",
            "step : 76.65 % , loss : 5.09906005859375\n",
            "step : 77.09 % , loss : 5.127832889556885\n",
            "step : 77.53 % , loss : 5.026462078094482\n",
            "step : 77.97 % , loss : 5.095934867858887\n",
            "step : 78.41 % , loss : 5.1785149574279785\n",
            "step : 78.85 % , loss : 5.253471374511719\n",
            "step : 79.3 % , loss : 5.030145168304443\n",
            "step : 79.74 % , loss : 5.145802021026611\n",
            "step : 80.18 % , loss : 5.0428290367126465\n",
            "step : 80.62 % , loss : 5.150484561920166\n",
            "step : 81.06 % , loss : 5.134734153747559\n",
            "step : 81.5 % , loss : 5.080270767211914\n",
            "step : 81.94 % , loss : 5.146054744720459\n",
            "step : 82.38 % , loss : 5.101713180541992\n",
            "step : 82.82 % , loss : 5.087831497192383\n",
            "step : 83.26 % , loss : 5.1083598136901855\n",
            "step : 83.7 % , loss : 5.099982738494873\n",
            "step : 84.14 % , loss : 5.147679805755615\n",
            "step : 84.58 % , loss : 5.123483657836914\n",
            "step : 85.02 % , loss : 5.082695007324219\n",
            "step : 85.46 % , loss : 5.0639967918396\n",
            "step : 85.9 % , loss : 5.023098468780518\n",
            "step : 86.34 % , loss : 5.130074501037598\n",
            "step : 86.78 % , loss : 5.0358567237854\n",
            "step : 87.22 % , loss : 5.166642189025879\n",
            "step : 87.67 % , loss : 5.042489051818848\n",
            "step : 88.11 % , loss : 5.241309642791748\n",
            "step : 88.55 % , loss : 5.123234748840332\n",
            "step : 88.99 % , loss : 5.143287658691406\n",
            "step : 89.43 % , loss : 5.0853095054626465\n",
            "step : 89.87 % , loss : 5.268168926239014\n",
            "step : 90.31 % , loss : 5.157803535461426\n",
            "step : 90.75 % , loss : 5.042219638824463\n",
            "step : 91.19 % , loss : 5.222029685974121\n",
            "step : 91.63 % , loss : 5.129868507385254\n",
            "step : 92.07 % , loss : 5.17258358001709\n",
            "step : 92.51 % , loss : 4.951035499572754\n",
            "step : 92.95 % , loss : 5.060981273651123\n",
            "step : 93.39 % , loss : 5.0983428955078125\n",
            "step : 93.83 % , loss : 5.175694465637207\n",
            "step : 94.27 % , loss : 5.063841819763184\n",
            "step : 94.71 % , loss : 5.040015697479248\n",
            "step : 95.15 % , loss : 5.163825988769531\n",
            "step : 95.59 % , loss : 5.22404146194458\n",
            "step : 96.04 % , loss : 5.120330810546875\n",
            "step : 96.48 % , loss : 5.06657600402832\n",
            "step : 96.92 % , loss : 5.0476202964782715\n",
            "step : 97.36 % , loss : 5.023934841156006\n",
            "step : 97.8 % , loss : 5.174945831298828\n",
            "step : 98.24 % , loss : 5.016205310821533\n",
            "step : 98.68 % , loss : 5.089517116546631\n",
            "step : 99.12 % , loss : 5.071998119354248\n",
            "step : 99.56 % , loss : 5.0796427726745605\n",
            "Epoch: 8 | Time: 1m 34s\n",
            "\tTrain Loss: 5.121 | Train PPL: 167.546\n",
            "\tVal Loss: 4.940 |  Val PPL: 139.731\n",
            "\tBLEU Score: 10.976\n",
            "step : 0.0 % , loss : 5.080112457275391\n",
            "step : 0.44 % , loss : 5.109029769897461\n",
            "step : 0.88 % , loss : 4.950123310089111\n",
            "step : 1.32 % , loss : 5.06616735458374\n",
            "step : 1.76 % , loss : 5.099287033081055\n",
            "step : 2.2 % , loss : 5.0511627197265625\n",
            "step : 2.64 % , loss : 5.029313087463379\n",
            "step : 3.08 % , loss : 5.064088344573975\n",
            "step : 3.52 % , loss : 5.212687969207764\n",
            "step : 3.96 % , loss : 5.027292728424072\n",
            "step : 4.41 % , loss : 4.997171401977539\n",
            "step : 4.85 % , loss : 5.124458312988281\n",
            "step : 5.29 % , loss : 5.05678653717041\n",
            "step : 5.73 % , loss : 5.090254306793213\n",
            "step : 6.17 % , loss : 5.196307182312012\n",
            "step : 6.61 % , loss : 5.041867733001709\n",
            "step : 7.05 % , loss : 4.946815490722656\n",
            "step : 7.49 % , loss : 5.134843349456787\n",
            "step : 7.93 % , loss : 5.039706230163574\n",
            "step : 8.37 % , loss : 5.131425380706787\n",
            "step : 8.81 % , loss : 5.03602409362793\n",
            "step : 9.25 % , loss : 5.119894504547119\n",
            "step : 9.69 % , loss : 5.064145565032959\n",
            "step : 10.13 % , loss : 5.144766807556152\n",
            "step : 10.57 % , loss : 5.019158363342285\n",
            "step : 11.01 % , loss : 5.035505771636963\n",
            "step : 11.45 % , loss : 5.159794807434082\n",
            "step : 11.89 % , loss : 5.122193336486816\n",
            "step : 12.33 % , loss : 5.079418182373047\n",
            "step : 12.78 % , loss : 5.132542133331299\n",
            "step : 13.22 % , loss : 5.0726237297058105\n",
            "step : 13.66 % , loss : 5.041632652282715\n",
            "step : 14.1 % , loss : 5.090522289276123\n",
            "step : 14.54 % , loss : 4.976995944976807\n",
            "step : 14.98 % , loss : 5.123932838439941\n",
            "step : 15.42 % , loss : 5.149091720581055\n",
            "step : 15.86 % , loss : 4.975431442260742\n",
            "step : 16.3 % , loss : 5.108406066894531\n",
            "step : 16.74 % , loss : 5.072196006774902\n",
            "step : 17.18 % , loss : 5.178234100341797\n",
            "step : 17.62 % , loss : 5.015837669372559\n",
            "step : 18.06 % , loss : 5.103654384613037\n",
            "step : 18.5 % , loss : 5.077932834625244\n",
            "step : 18.94 % , loss : 5.180112838745117\n",
            "step : 19.38 % , loss : 5.184926509857178\n",
            "step : 19.82 % , loss : 5.086500644683838\n",
            "step : 20.26 % , loss : 5.175099849700928\n",
            "step : 20.7 % , loss : 5.175475597381592\n",
            "step : 21.15 % , loss : 5.12222957611084\n",
            "step : 21.59 % , loss : 5.029054641723633\n",
            "step : 22.03 % , loss : 5.108424663543701\n",
            "step : 22.47 % , loss : 5.160305500030518\n",
            "step : 22.91 % , loss : 5.209061145782471\n",
            "step : 23.35 % , loss : 5.070780277252197\n",
            "step : 23.79 % , loss : 5.075036525726318\n",
            "step : 24.23 % , loss : 5.104391574859619\n",
            "step : 24.67 % , loss : 4.9577531814575195\n",
            "step : 25.11 % , loss : 5.029747486114502\n",
            "step : 25.55 % , loss : 5.031552314758301\n",
            "step : 25.99 % , loss : 4.968358516693115\n",
            "step : 26.43 % , loss : 5.025973796844482\n",
            "step : 26.87 % , loss : 5.108333587646484\n",
            "step : 27.31 % , loss : 5.0282206535339355\n",
            "step : 27.75 % , loss : 5.035837173461914\n",
            "step : 28.19 % , loss : 5.143126487731934\n",
            "step : 28.63 % , loss : 5.193935394287109\n",
            "step : 29.07 % , loss : 5.070878028869629\n",
            "step : 29.52 % , loss : 5.0916266441345215\n",
            "step : 29.96 % , loss : 5.054775714874268\n",
            "step : 30.4 % , loss : 5.042423248291016\n",
            "step : 30.84 % , loss : 5.025198936462402\n",
            "step : 31.28 % , loss : 4.983048915863037\n",
            "step : 31.72 % , loss : 5.065439224243164\n",
            "step : 32.16 % , loss : 5.003294944763184\n",
            "step : 32.6 % , loss : 4.984374046325684\n",
            "step : 33.04 % , loss : 5.00786018371582\n",
            "step : 33.48 % , loss : 4.928457736968994\n",
            "step : 33.92 % , loss : 5.087419033050537\n",
            "step : 34.36 % , loss : 5.036777973175049\n",
            "step : 34.8 % , loss : 5.025951385498047\n",
            "step : 35.24 % , loss : 5.120800495147705\n",
            "step : 35.68 % , loss : 4.957468509674072\n",
            "step : 36.12 % , loss : 5.111979961395264\n",
            "step : 36.56 % , loss : 5.178626537322998\n",
            "step : 37.0 % , loss : 5.071160316467285\n",
            "step : 37.44 % , loss : 5.040823936462402\n",
            "step : 37.89 % , loss : 5.1237874031066895\n",
            "step : 38.33 % , loss : 5.003089904785156\n",
            "step : 38.77 % , loss : 4.977383613586426\n",
            "step : 39.21 % , loss : 5.041979789733887\n",
            "step : 39.65 % , loss : 5.039490699768066\n",
            "step : 40.09 % , loss : 5.1442694664001465\n",
            "step : 40.53 % , loss : 5.102148532867432\n",
            "step : 40.97 % , loss : 5.101356029510498\n",
            "step : 41.41 % , loss : 5.076707363128662\n",
            "step : 41.85 % , loss : 5.150925159454346\n",
            "step : 42.29 % , loss : 5.00658655166626\n",
            "step : 42.73 % , loss : 5.03159761428833\n",
            "step : 43.17 % , loss : 4.963818550109863\n",
            "step : 43.61 % , loss : 5.055481910705566\n",
            "step : 44.05 % , loss : 5.066066265106201\n",
            "step : 44.49 % , loss : 5.040555000305176\n",
            "step : 44.93 % , loss : 4.954305648803711\n",
            "step : 45.37 % , loss : 5.074819087982178\n",
            "step : 45.81 % , loss : 5.008828163146973\n",
            "step : 46.26 % , loss : 4.989853858947754\n",
            "step : 46.7 % , loss : 5.019807815551758\n",
            "step : 47.14 % , loss : 5.083631992340088\n",
            "step : 47.58 % , loss : 4.951500415802002\n",
            "step : 48.02 % , loss : 5.0153374671936035\n",
            "step : 48.46 % , loss : 5.08036994934082\n",
            "step : 48.9 % , loss : 5.060844421386719\n",
            "step : 49.34 % , loss : 4.9807915687561035\n",
            "step : 49.78 % , loss : 5.078479290008545\n",
            "step : 50.22 % , loss : 5.0610551834106445\n",
            "step : 50.66 % , loss : 5.001437664031982\n",
            "step : 51.1 % , loss : 5.1494140625\n",
            "step : 51.54 % , loss : 5.027091026306152\n",
            "step : 51.98 % , loss : 5.0834784507751465\n",
            "step : 52.42 % , loss : 5.136425971984863\n",
            "step : 52.86 % , loss : 5.039105415344238\n",
            "step : 53.3 % , loss : 4.972559928894043\n",
            "step : 53.74 % , loss : 4.959993362426758\n",
            "step : 54.19 % , loss : 5.05263614654541\n",
            "step : 54.63 % , loss : 5.017789363861084\n",
            "step : 55.07 % , loss : 5.085309982299805\n",
            "step : 55.51 % , loss : 4.965321063995361\n",
            "step : 55.95 % , loss : 5.011264324188232\n",
            "step : 56.39 % , loss : 5.156849384307861\n",
            "step : 56.83 % , loss : 5.11373233795166\n",
            "step : 57.27 % , loss : 5.021608829498291\n",
            "step : 57.71 % , loss : 4.990325450897217\n",
            "step : 58.15 % , loss : 4.997014045715332\n",
            "step : 58.59 % , loss : 4.966390609741211\n",
            "step : 59.03 % , loss : 5.037604331970215\n",
            "step : 59.47 % , loss : 5.062366485595703\n",
            "step : 59.91 % , loss : 4.95604133605957\n",
            "step : 60.35 % , loss : 4.982532024383545\n",
            "step : 60.79 % , loss : 5.0202250480651855\n",
            "step : 61.23 % , loss : 5.055233478546143\n",
            "step : 61.67 % , loss : 5.094409942626953\n",
            "step : 62.11 % , loss : 4.948444366455078\n",
            "step : 62.56 % , loss : 5.0157856941223145\n",
            "step : 63.0 % , loss : 4.889283180236816\n",
            "step : 63.44 % , loss : 4.9831366539001465\n",
            "step : 63.88 % , loss : 4.998485088348389\n",
            "step : 64.32 % , loss : 5.026337146759033\n",
            "step : 64.76 % , loss : 5.029944896697998\n",
            "step : 65.2 % , loss : 5.02936315536499\n",
            "step : 65.64 % , loss : 4.829352855682373\n",
            "step : 66.08 % , loss : 4.958050727844238\n",
            "step : 66.52 % , loss : 5.0640482902526855\n",
            "step : 66.96 % , loss : 4.923863410949707\n",
            "step : 67.4 % , loss : 5.096945762634277\n",
            "step : 67.84 % , loss : 4.910237789154053\n",
            "step : 68.28 % , loss : 5.1137800216674805\n",
            "step : 68.72 % , loss : 5.076994895935059\n",
            "step : 69.16 % , loss : 4.995018005371094\n",
            "step : 69.6 % , loss : 5.06585168838501\n",
            "step : 70.04 % , loss : 5.029801368713379\n",
            "step : 70.48 % , loss : 5.004880905151367\n",
            "step : 70.93 % , loss : 5.047715663909912\n",
            "step : 71.37 % , loss : 5.057830333709717\n",
            "step : 71.81 % , loss : 4.9381489753723145\n",
            "step : 72.25 % , loss : 5.021908283233643\n",
            "step : 72.69 % , loss : 5.073416233062744\n",
            "step : 73.13 % , loss : 5.014029502868652\n",
            "step : 73.57 % , loss : 5.136972904205322\n",
            "step : 74.01 % , loss : 4.908883571624756\n",
            "step : 74.45 % , loss : 4.949242115020752\n",
            "step : 74.89 % , loss : 5.054502487182617\n",
            "step : 75.33 % , loss : 4.92777156829834\n",
            "step : 75.77 % , loss : 5.014737129211426\n",
            "step : 76.21 % , loss : 4.969064235687256\n",
            "step : 76.65 % , loss : 5.027975559234619\n",
            "step : 77.09 % , loss : 5.0435075759887695\n",
            "step : 77.53 % , loss : 5.0953264236450195\n",
            "step : 77.97 % , loss : 4.967016696929932\n",
            "step : 78.41 % , loss : 5.056100368499756\n",
            "step : 78.85 % , loss : 5.014833450317383\n",
            "step : 79.3 % , loss : 4.993617534637451\n",
            "step : 79.74 % , loss : 4.8810577392578125\n",
            "step : 80.18 % , loss : 5.103623867034912\n",
            "step : 80.62 % , loss : 5.115109443664551\n",
            "step : 81.06 % , loss : 4.9818644523620605\n",
            "step : 81.5 % , loss : 4.993929862976074\n",
            "step : 81.94 % , loss : 5.043756008148193\n",
            "step : 82.38 % , loss : 4.956238269805908\n",
            "step : 82.82 % , loss : 5.093484878540039\n",
            "step : 83.26 % , loss : 4.938790798187256\n",
            "step : 83.7 % , loss : 4.997344493865967\n",
            "step : 84.14 % , loss : 5.074507713317871\n",
            "step : 84.58 % , loss : 5.063138961791992\n",
            "step : 85.02 % , loss : 5.066895008087158\n",
            "step : 85.46 % , loss : 4.8961615562438965\n",
            "step : 85.9 % , loss : 4.988025188446045\n",
            "step : 86.34 % , loss : 5.113301753997803\n",
            "step : 86.78 % , loss : 5.069543838500977\n",
            "step : 87.22 % , loss : 5.031365871429443\n",
            "step : 87.67 % , loss : 5.026564598083496\n",
            "step : 88.11 % , loss : 4.9745378494262695\n",
            "step : 88.55 % , loss : 5.029131889343262\n",
            "step : 88.99 % , loss : 4.9643778800964355\n",
            "step : 89.43 % , loss : 5.028072834014893\n",
            "step : 89.87 % , loss : 5.053738117218018\n",
            "step : 90.31 % , loss : 4.950221061706543\n",
            "step : 90.75 % , loss : 5.061801910400391\n",
            "step : 91.19 % , loss : 5.024566650390625\n",
            "step : 91.63 % , loss : 4.9688720703125\n",
            "step : 92.07 % , loss : 4.926466941833496\n",
            "step : 92.51 % , loss : 4.960834503173828\n",
            "step : 92.95 % , loss : 4.94631814956665\n",
            "step : 93.39 % , loss : 4.834145545959473\n",
            "step : 93.83 % , loss : 5.004157066345215\n",
            "step : 94.27 % , loss : 4.9362406730651855\n",
            "step : 94.71 % , loss : 5.020225524902344\n",
            "step : 95.15 % , loss : 5.036517143249512\n",
            "step : 95.59 % , loss : 4.976527214050293\n",
            "step : 96.04 % , loss : 5.003882884979248\n",
            "step : 96.48 % , loss : 5.040646553039551\n",
            "step : 96.92 % , loss : 5.056068420410156\n",
            "step : 97.36 % , loss : 4.960631847381592\n",
            "step : 97.8 % , loss : 5.067203998565674\n",
            "step : 98.24 % , loss : 4.912289142608643\n",
            "step : 98.68 % , loss : 4.945692539215088\n",
            "step : 99.12 % , loss : 5.011778354644775\n",
            "step : 99.56 % , loss : 4.850800037384033\n",
            "Epoch: 9 | Time: 1m 34s\n",
            "\tTrain Loss: 5.039 | Train PPL: 154.392\n",
            "\tVal Loss: 4.814 |  Val PPL: 123.275\n",
            "\tBLEU Score: 12.296\n",
            "step : 0.0 % , loss : 4.978121280670166\n",
            "step : 0.44 % , loss : 5.059245586395264\n",
            "step : 0.88 % , loss : 5.029519081115723\n",
            "step : 1.32 % , loss : 5.0565571784973145\n",
            "step : 1.76 % , loss : 4.926881790161133\n",
            "step : 2.2 % , loss : 4.867896556854248\n",
            "step : 2.64 % , loss : 4.962249279022217\n",
            "step : 3.08 % , loss : 4.990902423858643\n",
            "step : 3.52 % , loss : 5.083469867706299\n",
            "step : 3.96 % , loss : 5.0466389656066895\n",
            "step : 4.41 % , loss : 4.947327136993408\n",
            "step : 4.85 % , loss : 4.958214282989502\n",
            "step : 5.29 % , loss : 5.122976779937744\n",
            "step : 5.73 % , loss : 4.982955455780029\n",
            "step : 6.17 % , loss : 4.869594097137451\n",
            "step : 6.61 % , loss : 5.027196884155273\n",
            "step : 7.05 % , loss : 5.124782085418701\n",
            "step : 7.49 % , loss : 4.848085403442383\n",
            "step : 7.93 % , loss : 4.928713321685791\n",
            "step : 8.37 % , loss : 4.98696231842041\n",
            "step : 8.81 % , loss : 4.93687629699707\n",
            "step : 9.25 % , loss : 5.0387420654296875\n",
            "step : 9.69 % , loss : 4.989528656005859\n",
            "step : 10.13 % , loss : 4.998351573944092\n",
            "step : 10.57 % , loss : 4.921074867248535\n",
            "step : 11.01 % , loss : 5.069878578186035\n",
            "step : 11.45 % , loss : 4.974508762359619\n",
            "step : 11.89 % , loss : 5.067141056060791\n",
            "step : 12.33 % , loss : 5.0158467292785645\n",
            "step : 12.78 % , loss : 4.959634780883789\n",
            "step : 13.22 % , loss : 5.008738040924072\n",
            "step : 13.66 % , loss : 4.961106777191162\n",
            "step : 14.1 % , loss : 5.058973789215088\n",
            "step : 14.54 % , loss : 5.030669689178467\n",
            "step : 14.98 % , loss : 4.967065334320068\n",
            "step : 15.42 % , loss : 4.868056774139404\n",
            "step : 15.86 % , loss : 5.003265857696533\n",
            "step : 16.3 % , loss : 4.94881010055542\n",
            "step : 16.74 % , loss : 5.001107692718506\n",
            "step : 17.18 % , loss : 5.023917198181152\n",
            "step : 17.62 % , loss : 4.9457316398620605\n",
            "step : 18.06 % , loss : 4.923699378967285\n",
            "step : 18.5 % , loss : 4.947460174560547\n",
            "step : 18.94 % , loss : 4.893840312957764\n",
            "step : 19.38 % , loss : 5.021412372589111\n",
            "step : 19.82 % , loss : 4.992800235748291\n",
            "step : 20.26 % , loss : 4.902802467346191\n",
            "step : 20.7 % , loss : 4.956273555755615\n",
            "step : 21.15 % , loss : 4.90841817855835\n",
            "step : 21.59 % , loss : 4.918923854827881\n",
            "step : 22.03 % , loss : 4.925561904907227\n",
            "step : 22.47 % , loss : 4.929559230804443\n",
            "step : 22.91 % , loss : 4.977907657623291\n",
            "step : 23.35 % , loss : 5.046965599060059\n",
            "step : 23.79 % , loss : 4.922236442565918\n",
            "step : 24.23 % , loss : 5.005317687988281\n",
            "step : 24.67 % , loss : 5.031707286834717\n",
            "step : 25.11 % , loss : 5.017578601837158\n",
            "step : 25.55 % , loss : 4.908770561218262\n",
            "step : 25.99 % , loss : 5.023471832275391\n",
            "step : 26.43 % , loss : 4.962520599365234\n",
            "step : 26.87 % , loss : 4.927023887634277\n",
            "step : 27.31 % , loss : 5.066908359527588\n",
            "step : 27.75 % , loss : 5.04161262512207\n",
            "step : 28.19 % , loss : 4.870563507080078\n",
            "step : 28.63 % , loss : 5.075246334075928\n",
            "step : 29.07 % , loss : 5.028887748718262\n",
            "step : 29.52 % , loss : 4.984865188598633\n",
            "step : 29.96 % , loss : 4.912652969360352\n",
            "step : 30.4 % , loss : 4.938783645629883\n",
            "step : 30.84 % , loss : 4.989906311035156\n",
            "step : 31.28 % , loss : 4.967848777770996\n",
            "step : 31.72 % , loss : 4.897868633270264\n",
            "step : 32.16 % , loss : 4.975320816040039\n",
            "step : 32.6 % , loss : 5.008769989013672\n",
            "step : 33.04 % , loss : 4.975431442260742\n",
            "step : 33.48 % , loss : 5.111576557159424\n",
            "step : 33.92 % , loss : 4.9322662353515625\n",
            "step : 34.36 % , loss : 5.063440322875977\n",
            "step : 34.8 % , loss : 4.924572944641113\n",
            "step : 35.24 % , loss : 5.036677837371826\n",
            "step : 35.68 % , loss : 4.993674278259277\n",
            "step : 36.12 % , loss : 4.9298930168151855\n",
            "step : 36.56 % , loss : 5.085885047912598\n",
            "step : 37.0 % , loss : 4.996655464172363\n",
            "step : 37.44 % , loss : 4.85330867767334\n",
            "step : 37.89 % , loss : 4.952497959136963\n",
            "step : 38.33 % , loss : 5.022982120513916\n",
            "step : 38.77 % , loss : 4.970493316650391\n",
            "step : 39.21 % , loss : 4.907021522521973\n",
            "step : 39.65 % , loss : 4.985464572906494\n",
            "step : 40.09 % , loss : 5.041092395782471\n",
            "step : 40.53 % , loss : 4.918227195739746\n",
            "step : 40.97 % , loss : 4.910479545593262\n",
            "step : 41.41 % , loss : 4.841109752655029\n",
            "step : 41.85 % , loss : 4.959062099456787\n",
            "step : 42.29 % , loss : 4.984823703765869\n",
            "step : 42.73 % , loss : 4.944317817687988\n",
            "step : 43.17 % , loss : 4.977605819702148\n",
            "step : 43.61 % , loss : 5.002283573150635\n",
            "step : 44.05 % , loss : 4.975467205047607\n",
            "step : 44.49 % , loss : 5.009828090667725\n",
            "step : 44.93 % , loss : 4.905860900878906\n",
            "step : 45.37 % , loss : 4.961272716522217\n",
            "step : 45.81 % , loss : 5.010189056396484\n",
            "step : 46.26 % , loss : 4.957883834838867\n",
            "step : 46.7 % , loss : 4.90863561630249\n",
            "step : 47.14 % , loss : 4.975982666015625\n",
            "step : 47.58 % , loss : 4.9510393142700195\n",
            "step : 48.02 % , loss : 5.00160026550293\n",
            "step : 48.46 % , loss : 5.0625786781311035\n",
            "step : 48.9 % , loss : 4.929998874664307\n",
            "step : 49.34 % , loss : 4.9517998695373535\n",
            "step : 49.78 % , loss : 4.990300178527832\n",
            "step : 50.22 % , loss : 4.989276885986328\n",
            "step : 50.66 % , loss : 4.965266704559326\n",
            "step : 51.1 % , loss : 4.901119709014893\n",
            "step : 51.54 % , loss : 4.994594573974609\n",
            "step : 51.98 % , loss : 4.965989112854004\n",
            "step : 52.42 % , loss : 4.921726703643799\n",
            "step : 52.86 % , loss : 4.9743828773498535\n",
            "step : 53.3 % , loss : 4.927802562713623\n",
            "step : 53.74 % , loss : 4.8890910148620605\n",
            "step : 54.19 % , loss : 4.933871269226074\n",
            "step : 54.63 % , loss : 4.896306037902832\n",
            "step : 55.07 % , loss : 5.020519733428955\n",
            "step : 55.51 % , loss : 4.994471073150635\n",
            "step : 55.95 % , loss : 5.022372722625732\n",
            "step : 56.39 % , loss : 4.910378456115723\n",
            "step : 56.83 % , loss : 4.864826679229736\n",
            "step : 57.27 % , loss : 5.014026165008545\n",
            "step : 57.71 % , loss : 4.9846014976501465\n",
            "step : 58.15 % , loss : 4.902492046356201\n",
            "step : 58.59 % , loss : 4.923782825469971\n",
            "step : 59.03 % , loss : 4.925754070281982\n",
            "step : 59.47 % , loss : 4.8731818199157715\n",
            "step : 59.91 % , loss : 5.009481906890869\n",
            "step : 60.35 % , loss : 4.864646911621094\n",
            "step : 60.79 % , loss : 5.046372413635254\n",
            "step : 61.23 % , loss : 4.887659072875977\n",
            "step : 61.67 % , loss : 4.929088592529297\n",
            "step : 62.11 % , loss : 5.120642185211182\n",
            "step : 62.56 % , loss : 4.950967788696289\n",
            "step : 63.0 % , loss : 4.980640411376953\n",
            "step : 63.44 % , loss : 4.870256423950195\n",
            "step : 63.88 % , loss : 4.9280619621276855\n",
            "step : 64.32 % , loss : 5.003052234649658\n",
            "step : 64.76 % , loss : 4.988180160522461\n",
            "step : 65.2 % , loss : 4.845317363739014\n",
            "step : 65.64 % , loss : 4.9754109382629395\n",
            "step : 66.08 % , loss : 4.984975337982178\n",
            "step : 66.52 % , loss : 4.893113613128662\n",
            "step : 66.96 % , loss : 4.892334938049316\n",
            "step : 67.4 % , loss : 4.973653316497803\n",
            "step : 67.84 % , loss : 5.046951770782471\n",
            "step : 68.28 % , loss : 5.048540115356445\n",
            "step : 68.72 % , loss : 4.931959629058838\n",
            "step : 69.16 % , loss : 5.071490287780762\n",
            "step : 69.6 % , loss : 4.969932556152344\n",
            "step : 70.04 % , loss : 4.952724456787109\n",
            "step : 70.48 % , loss : 5.008145809173584\n",
            "step : 70.93 % , loss : 4.954215049743652\n",
            "step : 71.37 % , loss : 5.08409309387207\n",
            "step : 71.81 % , loss : 5.019245147705078\n",
            "step : 72.25 % , loss : 4.920753479003906\n",
            "step : 72.69 % , loss : 4.996986389160156\n",
            "step : 73.13 % , loss : 5.081958770751953\n",
            "step : 73.57 % , loss : 4.94146728515625\n",
            "step : 74.01 % , loss : 4.984241485595703\n",
            "step : 74.45 % , loss : 4.923324108123779\n",
            "step : 74.89 % , loss : 4.978487968444824\n",
            "step : 75.33 % , loss : 4.9170451164245605\n",
            "step : 75.77 % , loss : 4.9494948387146\n",
            "step : 76.21 % , loss : 4.964408874511719\n",
            "step : 76.65 % , loss : 5.058022975921631\n",
            "step : 77.09 % , loss : 4.937444686889648\n",
            "step : 77.53 % , loss : 5.029799938201904\n",
            "step : 77.97 % , loss : 4.942953586578369\n",
            "step : 78.41 % , loss : 4.9693708419799805\n",
            "step : 78.85 % , loss : 5.007450580596924\n",
            "step : 79.3 % , loss : 4.944777488708496\n",
            "step : 79.74 % , loss : 4.98680305480957\n",
            "step : 80.18 % , loss : 5.010257244110107\n",
            "step : 80.62 % , loss : 4.891942977905273\n",
            "step : 81.06 % , loss : 4.870576858520508\n",
            "step : 81.5 % , loss : 4.888263702392578\n",
            "step : 81.94 % , loss : 4.906191825866699\n",
            "step : 82.38 % , loss : 5.031347751617432\n",
            "step : 82.82 % , loss : 4.9223103523254395\n",
            "step : 83.26 % , loss : 4.914831638336182\n",
            "step : 83.7 % , loss : 4.946815490722656\n",
            "step : 84.14 % , loss : 4.935725212097168\n",
            "step : 84.58 % , loss : 4.858276844024658\n",
            "step : 85.02 % , loss : 4.981123924255371\n",
            "step : 85.46 % , loss : 4.962533950805664\n",
            "step : 85.9 % , loss : 4.882092475891113\n",
            "step : 86.34 % , loss : 5.009807109832764\n",
            "step : 86.78 % , loss : 5.053528785705566\n",
            "step : 87.22 % , loss : 4.827934265136719\n",
            "step : 87.67 % , loss : 4.9446210861206055\n",
            "step : 88.11 % , loss : 5.002781867980957\n",
            "step : 88.55 % , loss : 4.9683966636657715\n",
            "step : 88.99 % , loss : 5.009541988372803\n",
            "step : 89.43 % , loss : 4.915598392486572\n",
            "step : 89.87 % , loss : 4.9920454025268555\n",
            "step : 90.31 % , loss : 5.046154022216797\n",
            "step : 90.75 % , loss : 4.895505428314209\n",
            "step : 91.19 % , loss : 4.966297626495361\n",
            "step : 91.63 % , loss : 4.985965251922607\n",
            "step : 92.07 % , loss : 4.926643371582031\n",
            "step : 92.51 % , loss : 4.925939559936523\n",
            "step : 92.95 % , loss : 4.885351181030273\n",
            "step : 93.39 % , loss : 5.029457092285156\n",
            "step : 93.83 % , loss : 4.861365795135498\n",
            "step : 94.27 % , loss : 4.899113655090332\n",
            "step : 94.71 % , loss : 4.941957950592041\n",
            "step : 95.15 % , loss : 4.896307945251465\n",
            "step : 95.59 % , loss : 4.856934070587158\n",
            "step : 96.04 % , loss : 4.952820301055908\n",
            "step : 96.48 % , loss : 5.020920276641846\n",
            "step : 96.92 % , loss : 4.962525367736816\n",
            "step : 97.36 % , loss : 4.9999680519104\n",
            "step : 97.8 % , loss : 4.9302897453308105\n",
            "step : 98.24 % , loss : 4.867188453674316\n",
            "step : 98.68 % , loss : 4.903287410736084\n",
            "step : 99.12 % , loss : 4.938336372375488\n",
            "step : 99.56 % , loss : 4.951972007751465\n",
            "Epoch: 10 | Time: 1m 34s\n",
            "\tTrain Loss: 4.966 | Train PPL: 143.487\n",
            "\tVal Loss: 4.793 |  Val PPL: 120.671\n",
            "\tBLEU Score: 12.034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "ZylAdSzCIHG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "\n",
        "def read(name):\n",
        "    f = open(name, 'r')\n",
        "    file = f.read()\n",
        "    file = re.sub('\\\\[', '', file)\n",
        "    file = re.sub('\\\\]', '', file)\n",
        "    f.close()\n",
        "\n",
        "    return [float(i) for idx, i in enumerate(file.split(','))]\n",
        "\n",
        "\n",
        "def draw(mode):\n",
        "    if mode == 'loss':\n",
        "        train = read('./result/train_loss.txt')\n",
        "        test = read('./result/test_loss.txt')\n",
        "        plt.plot(train, 'r', label='train')\n",
        "        plt.plot(test, 'b', label='validation')\n",
        "        plt.legend(loc='lower left')\n",
        "\n",
        "\n",
        "    elif mode == 'bleu':\n",
        "        bleu = read('./result/bleu.txt')\n",
        "        plt.plot(bleu, 'b', label='bleu score')\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel(mode)\n",
        "    plt.title('training result')\n",
        "    plt.grid(True, which='both', axis='both')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "draw(mode='loss')\n",
        "draw(mode='bleu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "l3X_DcpgIKED",
        "outputId": "cd9acbc7-f69c-40c0-a431-b90f14605162"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def inference(model, iterator):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg[:, :-1])\n",
        "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "\n",
        "            all_outputs = []\n",
        "            for j in range(batch_size):\n",
        "                try:\n",
        "                    trg_words = idx_to_word(batch.trg[j], loader.target.vocab)\n",
        "                    output_words = output[j].max(dim=1)[1]\n",
        "                    output_words = idx_to_word(output_words, loader.target.vocab)\n",
        "                    all_outputs.append(output_words)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "inference(model, valid_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHa2SjwsUzSo",
        "outputId": "3267f367-0c9f-4d5d-cef3-31b23c513ce5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ein mann in einem und und und und und und und und und und und , , , , , , , , , , , , , , , , ,',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , , , , , , , . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und und , , , , . . . . . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , , , , , , , , , , , . ,',\n",
              " 'ein mann in einem und und und und und und und und und und und , , , , , , , , , , , . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , , , , , . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , , , , , , , , , , , ,',\n",
              " 'ein mann in einem und und und und und und und und und und und , , , , , , . . . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , . . . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , , , , . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , , , . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , , , . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und und , , , , , , . . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , . . . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , . . . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und und , , , , , , . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , , , . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und und , , , , . . . . . . . . . . . .',\n",
              " 'ein mann in einem und und und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und und , , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und und , , , . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und und , , , . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , , . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und , . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem und und und und und und und und und und , , . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und , , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und , . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und und . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und , . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem und und und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem einem und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem einem und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und . . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem einem einem und . . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem einem und . . . . . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem und und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und und . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und . . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem und und und und und . . . . . . . . . . . . . . . . . . .',\n",
              " 'ein mann in einem einem einem einem einem einem und und und und . . . . . . . . . . . . . . . . . . .']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}